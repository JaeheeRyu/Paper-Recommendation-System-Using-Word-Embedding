Convolutional Neural Networks for Multi-class  

Histopathology Image Classification   

Muhammed Taloa* 

a Department of Computer Engineering, Munzur University, Tunceli, Turkey 

Abstract 

There is a strong need for automated systems to improve diagnostic quality and reduce the analysis 
time in histopathology image processing. Automated detection and classification of pathological 
tissue characteristics with computer-aided diagnostic systems are a critical step in the early 
diagnosis and treatment of diseases. Once a pathology image is scanned by a microscope and 
loaded onto a computer, it can be used for automated detection and classification of diseases. In 
this study, the DenseNet-161 and ResNet-50 pre-trained CNN models have been used to classify 
digital histopathology patches into the corresponding whole slide images via transfer learning 
technique. The proposed pre-trained models were tested on grayscale and color histopathology 
images. The DenseNet-161 pre-trained model achieved a classification accuracy of 97.89% using 
grayscale images and the ResNet-50 model obtained the accuracy of 98.87% for color images. The 
proposed pre-trained models outperform state-of-the-art methods in all performance metrics to 
classify digital pathology patches into 24 categories. 

Keywords: Medical imaging, digital pathology, deep learning, transfer learning, CNN. 

1 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
1. Introduction 

Pathologists manually examine tissue samples using a microscope to determine if a disease is 

present in a tissue. The diagnosis of pathological diseases has been performed by analyzing various 

tissue samples which usually obtained by a biopsy procedure [1]. The extracted tissue sections are 

stained to reveal its high-level structure and then placed on a glass slide in order to analyze the 

morphological structures [2].  

In recent years, the microscopes are being replaced by digital scanners. So, the entire histology 

slide is scanned by a digital whole slide scanner and saved as a whole slide image (WSI). The 

analysis of tissue characteristics from WSIs has opened a new era in digital pathology [3]. 

Computer-aided diagnostic (CAD) systems can perform the diagnosis and classification of the 

diseases using WSI images. However, the diagnosis of digital pathology images is a tedious task 

because of the large image size of the WSIs. The high-resolution WSIs are usually divided into 

partial patches and the analyses are performed individually on each sample.  

In the analysis of histological images, different interpretations can be seen among different 

pathologists [4]. Since the WSIs are very large in size, it is not easy for experts to diagnose disease 

types by looking at all patches. The computer-aided diagnosis systems can overcome these 

problems. Hence, there is a strong demand for the development of computer-assisted diagnostic 

tools that can reduce the workload of pathologists by helping them for a fast and precise diagnosis 

[5]. 

The convolutional neural networks (CNNs) is become a method of choice in medical image 

analysis. In recent years, deep learning architectures, especially CNNs are the most successful type 

of methods has been applied to medical image analysis tasks such as detection [6-9] and 

classification [10-11]. The CNN architectures have an end-to-end structure, which learns high-

level representations from the data [12]. Deep learning algorithms have also achieved tremendous 

success in pathology image processing, such as mitosis detection [13], tissue grading 

(classification) [14], and nuclei segmentation [15] from the high-resolution images. In 

histopathology image analysis, the color and texture based features of histology images are used 

for the segmentation and classification tasks [16]. 

2 

 
 
 
Several studies have been conducted for automated classification and segmentation of 

histopathology images using deep learning methods. Saha et al. [17], implemented a deep learning 

based model with handcrafted features (textual, morphological and intensity features) for 

automated detection of mitoses from breast histopathology WSIs. The authors reported a 92% 

precision and 88% recall for the proposed architecture. Han et al. [18] have implemented a 

structured deep learning model for multi-classification of breast cancer from histopathology 

images. They achieved an average of 93.2% classification accuracy to classify subclasses of breast 

cancer such as lobular carcinoma, ductal carcinoma, and fibroadenoma. Zheng et al. [19], 

developed a convolutional neural network to classify breast lesions. The authors used a two class 

(benign and malignant) breast tumor dataset and reached a classification accuracy of 96.6%. They 

also used a 15-class breast tumors dataset and reported 96.4% average classification accuracy. Jia 

et al. [20] proposed a fully connected network using multiple instance learning algorithm to 

segment cancerous regions in histopathological images. Komura et al. [21], presents a concise 

review of studies that use machine learning approaches for digital histopathology image analysis. 

The contribution of this study is fourfold: (1) An end-to-end pre-trained models were developed 

for automated classification of histopathology images. (2) The color and grayscale images are 

classified into 24 categories using all available data in Kimia Path24 dataset. (3) The classification 

performance of different pre-trained models was compared for both color and grayscale images. 

(4) The proposed pre-trained models demonstrate a better classification accuracy than the existing 

studies on the literature.  

The remainder of this paper is organized as follows.  The materials and methods are presented in 

the next section. The experiments and results are reported in Section 3 and discussions are given 

in Section 4. Finally, Section 6 presents the contributions and future works.  

2. Material and Methods 

2.1. Dataset 

The Kimia Path24 histopathology dataset used in this study was released by Babaie et al. [22], 

which contains 24 whole-slide tissue images. The dataset is publicly available and shows various 

body parts with different texture patterns. The images in the Kimia Path24 dataset were obtained 

3 

 
 
 
 
by TissueScope LE 1.0. The dataset contains a total of 23,916 images and 1325 of these images 

selected for testing. All images have an equal size of 1000Ã—1000 pixels. The selected test patches 

were extracted from the WSIs and their locations were stained to white on the scans. Thus, the use 

of test patches for the training set was prevented.  

The previous studies conducted on the Kimia Path24 dataset were converted the color images to 

grayscale and the classification performance was given on grayscale images. In this study, the 

classification of images is performed using both color and grayscale images in order to use the 

color and texture features. Few sample grayscale and color images from Kimia Path24 are shown 

in Fig.1.  

Figure 1. A few sample color and grayscale images from Kimia Path24 dataset. Each row shows the training and 

test images of color and grayscale images. 

4 

 
 
 
 
2.2. Deep transfer learning 

Convolutional neural networks automatically learn the best representative features from the raw 

data instead of using traditional machine learning techniques that benefit from the handcrafted 

features. A typical convolutional neural network consists of a series of building blocks such as 

convolutional layers, activation layers, pooling layers, and fully-connected layers. The shallow 

CNN architectures were constructed stacking a couple of the building blocks together like AlexNet 

[23] and VggNet [24]. However, the modern CNN architectures are deeper as compared to the 

shallow CNN architectures and use progressively more complex connections among alternating 

layers, such as ResNet [25] and DenseNet [26].  

In this study, the ResNet-50 and DenseNet-161 pre-trained CNN architectures are employed to 

classify digitalized pathology images into 24 different classes. ResNet and DenseNet architectures 

have been used in the construction of new state-of-the-art models. Both of these architectures 

trained on a subset of ImageNet database which contains 1.2 million images belongs to 1000 

classes.  

ResNet: The Residual neural networks (ResNet) was developed by He et al. [25], got the first 

place in the ImageNet Large Scale Visual Recognition Challenge (ILSVRC 2015) with 3.57% 

error rate. The authors used a 152 layer deep CNN architecture in the computation. The ResNet 

architecture popularized the idea of using deeper networks as compared to AlexNet, which has 

eight layers, and VggNet with up to 19 layers. The ResNet architecture introduced skip 

connections, also known as residual connections to avoid information loss during training of deep 

network. Skip connection technique enables to train very deep networks and can boost the 

performance of the model. The authors were able to train a 1001-layer deep model using residual 

connections. The ResNet architecture mainly composed of residual blocks. In shallow neural 

networks, consecutive hidden layers are linked to each other, however, in the ResNet architecture, 

there are also connections among residual blocks. The main compelling advantages of residual 

connections in ResNet architecture; the connections preserve the gained knowledge during training 

and speed up the training time of the model by increasing the capacity of the network. A block 

diagram of the pre-trained ResNet-50 model used in this study is shown in Fig.2. 

5 

 
 
 
 
Figure 2. A block diagram representation of pre-trained Resnet-50 architecture. 

DenseNet: Dense convolutional networks (DenseNet) developed by Huang, Liu and Maaten [26] 

had the best classification performance on publicly available image datasets such as CIFAR-10 

and ImageNet in 2017. The DenseNet architecture uses dense connections in the construction of 

the model, other than direct connections among the hidden layers of the network, as in the ResNet 

architecture. DenseNet was built with a structure that connects each layer to later layers. Hence, 

important features learned by any layers of the network is shared within the network. In other 

words, the extra links among the layers of DenseNet boost the information flow through the whole 

network. In this way, the training of the deep network becomes more efficient and the performance 

of the model increases.  The DenseNet architecture uses fewer parameters than similar CNN 

architectures for the training of the network because of the direct transmission of the feature maps 

to all subsequent layers. Additionally, using dense connections reduce the overfitting of the model 

on tasks which have smaller datasets. A block diagram of DenseNet-161 having four dense blocks 

is shown in Figure 3. 

Figure 3. The schematic representation of the DenseNet-161 architecture. 

6 

 
 
 
 
 
Training deep architectures with millions of parameters can take weeks using random weights 

initialization technique. A large amount of data and powerful computer hardware (GPUs) are 

required in order to train CNN from starch. The transfer learning technique is commonly used to 

elevate these problems. With this technique, a CNN model is trained on a huge dataset then the 

features learned from this model transfer to a model at hand. In the transfer learning technique, the 

fully connected layer of the model is removed and the remaining layers of the architecture are used 

as a feature extractor for the new task. Therefore, only the fully connected layers of the new model 

are trained in the current model. The schematic representation of transfer learning technique used 

in this study to classify histopathology images is given in Fig.4. 

Figure 4. The transfer learning method used to classify histopathology images. 

7 

 
 
 
 
 
 
3. Experiments and Results  

The Resnet-50 and DenseNet-161 pre-trained CNN architectures are employed to classify 

histopathology images into 24 classes. The official Kimia Path24 dataset consists of a total of 

23,916 images for training and 1325 images for testing.  The training set of 23,916 images are 

divided into 80% for training and 20% for the validation set in order to have a separate validation 

set.  The performances of the proposed pre-trained CNN models are evaluated on the test set.  

PyTorch [27] library and Fastai [28] framework used to implement the proposed pre-trained 

models. Both of these libraries are open source and build on Python library for machine learning 

tasks. The training and testing of the ResNet-50 and DenseNet-161 models were performed on 

NVIDIA GeForce GTX 1080 TI with 11 GB graphics memory. 

3.1 Experimental Setups 

In this study, the transfer learning technique is applied to the ResNet-50 and DensNet-161 pre-

trained CNN architectures. The fully connected layer of the DenseNet-161 and ResNet-50 were 

removed and the convolutional layers of these pre-trained models were used as a base network in 

the new architectures. Two sets of batch normalization, dropout, and fully connected layers are 

respectively added to the base network. The addition of two batch normalization layers is due to 

the rapid training of the pre-trained model. The dropout layers are attached to the base network to 

alleviate the overfitting problem. The overfitting problem causes the model to fail to generalize on 

the unseen test dataset. Further, in the last layer of proposed models, the Softmax activation 

function is used to classify the digital pathology images into 24 classes. The overall framework of 

the customized CNN architecture is given in Fig.5. 

Each of the pre-trained models was trained independently on color and grayscale datasets. All the 

models were trained for the same number of epochs. The training of CNN was performed only in 

the newly attached layers of the network. With this way, the computation cost of the network in 

the training process is decreased as compared to the training of the whole network.  

8 

 
 
 
 
 
 
Figure 5. An illustration of the customized CNN architecture. The first five feature extractor convolutional blocks 
which are shown inside the yellow box are used as the base of the network. The rest of the layers are added to the 
base network. 

The stochastic gradient descent (SGD) with momentum, namely RMSprop is employed to 

optimize the parameters of the network during training. The learning rate value is randomly chosen 

as 1e-3. The dropout ratios are selected as 0.25 and 0.50, respectively, to regularize the deep 

models. The batch normalization values were selected as 0.1 for momentum and 1e-5 for epsilon. 

The accuracy calculation protocol established by [22] is followed in this study to compare the 

results of this work with other studies conducted on the Kimia Path24 dataset. There are a total of 

1325 test patches, ğ‘›ğ‘¡ğ‘œğ‘¡ = 1325 from 24 distinct sets.  Let ğ‘… represent the set of retrieved images 

for an experiment and let  ğ›¤ğ‘  = {ğ‘ƒğ‘ 
ğ‘– |ğ‘  âˆˆ ğ‘† ,   ğ‘– = 1,2, â€¦ }, where ğ‘  = 0,1, â€¦ ,23,  the patch-to-scan 

accuracy, ğœ‚ ğ‘ is defined as:  

ğœ‚ ğ‘ = 

1

ğ‘›ğ‘¡ğ‘œğ‘¡

âˆ‘|ğ‘… âˆ© ğ›¤ğ‘  |

ğ‘ 

and the whole-scan accuracy,  ğœ‚ ğ‘¤ , is described as follows: 

The total accuracy is defined as:   

ğœ‚ ğ‘¤ = 

1
24

âˆ‘|ğ‘… âˆ© ğ›¤ğ‘  |

ğ‘ 

ğœ‚ ğ‘¡ğ‘œğ‘¡ğ‘ğ‘™ = ğœ‚ ğ‘ Ã— ğœ‚ ğ‘¤ 

9 

 
 
 
 
 
 
 
 
 
using both whole-scan and patch-to-scan accuracies [22]. The Python code for the accuracy 

calculations is publicly available and can be obtained from the Kimia Lab website [29]. 

3.2 Results 

The attached layers of the ResNet-50 and DenseNet-161 models were trained in an end-to-end 

fashion to classify histopathology images for 50 epochs. Each pre-trained model was trained 

separately on both color and grayscale images. For each grayscale and color datasets, the training 

accuracy and the training and validation loss graphs of pre-trained ResNet-50 and DenseNet-161 

are shown in Fig.5. 

Figure 5. The performance of DenseNet-161 and ResNet-50 architectures on grayscale and color datasets: (a) training 
accuracies of DenseNet-161 and ResNet-50 on grayscale images, (b) training and validation loss of ResNet-50 for 
grayscale images, (c) training and validation loss of DenseNet-161 for grayscale images, (d) training accuracies of 
ResNet-50 and DenseNet-161 on color dataset, (e) training and validation loss of ResNet -50 for color images, (f) 
training and validation loss of DenseNet-161 for color images. 

10 

ColorLossEpochs(f)LossEpochs(e)GrayscaleAccuracy05101520253035404550707580859095100Epochs(a)Epochs(b)LossEpochs(c)LossDenseNet-161ResNet-50DenseNet-161ResNet-50AccuracyEpochs(d)TrainValidationTrainValidationTrainValidationTrainValidationTrainValidationDenseNet-161ResNet-50 
 
 
 
 
 
During the training, DenseNet-161 outperforms the ResNet-50 for color images. However, for 

grayscale images, ResNet-50 obtained better validation accuracy than the DenseNet-161. Table 1 

shows the patch-to-scan (ğœ‚ ğ‘ ), whole scan (ğœ‚ ğ‘¤ ), and total (ğœ‚ ğ‘¡ğ‘œğ‘¡ğ‘ğ‘™ ) accuracies on test data and the 

total training time of each pre-trained CNN models for the color and grayscale images. 

Table 1. Classification results achieved on the test set using ResNet-50 and DenseNet-161 pre-trained models. 

Models 

Image format 

Training Time 
(hour : min : sec) 

DenseNet-161 

ResNet-50 
DenseNet-161 
ResNet-50 

grayscale 

grayscale 
RGB 
RGB 

4:14:37 

3:01:49 
4:23:50 
4:03:34 

ğœ¼ ğ’‘ 

(%) 
97.89 

96.08 
98.64 
98.87 

ğœ¼ ğ’˜ 

(%) 

97.86 

96.38 
98.63 
98.89 

ğœ¼ ğ’•ğ’ğ’•ğ’‚ğ’ 

(%) 

95.79 

92.60 
97.28 
97.77 

It can be seen from the Table-2 that; the DenseNet-161 achieves better classification accuracy than 

the ResNet-50 architecture in the classification of grayscale images on the test set. DenseNet-161 

architecture has developed the idea of shortcut connection among the layers of the network that 

also used by ResNet-50 architecture. DenseNet-161 architecture has more layers than ResNet50. 

However, surprisingly, in the grayscale images, ResNet-50 pre-trained model outperformed the 

DensNet-161 in the classification task.  The confusion matrixes of DenseNet-161 and ResNet-50 

pre-trained models obtained on grayscale and color test datasets (1325 images) are shown in Fig.6.  

Figure 6. The confusion matrixes are obtained using grayscale and color test datasets: (a) DenseNet-161, (b) 
ResNet-50.   

11 

650000000000000002000000106500000000000000000000000065000000000000000000000000740000000000000000006000001400000000000000000000000040120000100000000000000000681000000000000000000001004700000000000000000000000059000000000000100000000000600000002000000000000000007000000000000000000000000068000001000000000000000000690000000000000000000000006000000000000000000000000060010000000000000000000000270000000000000010000000004000000000000000000020000044000000000000000000000200250000000000000000000000002500000000000000000001000065000000000001000000000000640000010000000000000000005900000000000000000000000064Target ClassC1 C2 C3 C4 C5 C6 C7 C8 C9 C10C11C12C13C14C15C16C17C18C19C20C21C22C23C24Output ClassGrayscaleC1 C2 C3 C4 C5 C6 C7 C8 C9 C10C11C12C13C14C15C16C17C18C19C20C21C22C23C24650000100000000001000000206500000000000000000000000065000000000000000000000000730000000000000000001000001400000000000000000000000038120000000000000000000000690000000000000000000001104800000000000000010000000059000000000000000000000000590000000000000000000000006900000000000000000000000069000001000000000000000000700000000000000000000000006000000000000000000000000060000000000000000000000000300000000000000000010000004400000000000000000110000044000000000000000000000000250000000000000000000000002500000000000000000000000065000000000001000000000000650000020000000000000000006400000000000000000000000062Output ClassC1 C2 C3 C4 C5 C6 C7 C8 C9 C10C11C12C13C14C15C16C17C18C19C20C21C22C23C24Target ClassC1 C2 C3 C4 C5 C6 C7 C8 C9 C10C11C12C13C14C15C16C17C18C19C20C21C22C23C24Colora)b) 
 
 
 
Out of 1325 images, the DenseNet-161 model misclassified 25 images on grayscale test dataset 

and the ResNet-50 misclassified 18 histopathology images on the color test dataset. The precision, 

sensitivity, specificity, and F1-Score values of DenseNet-161 model on the grayscale dataset is 

given in Table 2 for the detailed performance analysis. 

Table 2. The classification report for DensNet-161 model using the grayscale test dataset. 

Classes 

     Class 1 
     Class 2 
     Class 3 
     Class 4 
     Class 5 
     Class 6 
     Class 7 
     Class 8 
     Class 9 
    Class 10 
    Class 11 
    Class 12 
    Class 13 
    Class 14 
    Class 15 
    Class 16 
    Class 17 
    Class 18 
    Class 19 
    Class 20 
    Class 21 
    Class 22 
    Class 23 
    Class 24 

Precision 
(%) 

0.96 
1.00 
1.00 
0.93 
1.00 
0.91 
0.99 
0.98 
0.98 
0.97 
1.00 
0.99 
1.00 
1.00 
0.98 
1.00 
0.98 
0.96 
0.93 
1.00 
0.98 
0.98 
0.98 
1.00 

Recall 

F1-score 

(%) 
1.00 
1.00 
1.00 
0.99 
0.93 
1.00 
0.97 
0.94 
0.98 
1.00 
1.00 
0.97 
0.99 
1.00 
1.00 
0.90 
0.89 
0.98 
1.00 
1.00 
1.00 
0.98 
0.91 
0.98 

(%) 
0.98 
1.00 
1.00 
0.95 
0.97 
0.95 
0.98 
0.96 
0.98 
0.98 
1.00 
0.98 
0.99 
1.00 
0.99 
0.95 
0.93 
0.97 
0.96 
1.00 
0.99 
0.98 
0.94 
0.99 

Amount of 
Data 

      65 
65 
65 
75 
15 
40 
70 
50 
60 
60 
70 
70 
70 
60 
60 
30 
45 
45 
25 
25 
65 
65 
65 
65 

The average precision, recall, and F1-score value for DenseNet-161 is 98% on the grayscale 

images and 99% for ResNet-50 on the color images. The analyses show that the performance of 

pre-trained models on the color dataset is better than the grayscale. This may be due to the reason 

that the ResNet-50 pre-trained model learned both color and textual features during training. 

However, DenseNet-161 model only learned the textual representations on grayscale data.  

12 

 
 
 
 
 
 
 
 
 
4. Discussion 

Different CNN models, pre-trained networks and feature extraction methods were used to classify 

histopathology images. Table 3 displays the comparison of the state-of-art studies applied to the 

Kimia Path24 dataset. 

Table 3. Accuracy comparison of this work with other studies that used the same dataset 

Papers 

Year 

Babaie et al. [22] 
Babaie et al. [22] 
Babaie et al. [22] 
Kieffer et al. [30] 
Kieffer et al. [30] 
Zhu et al. [31] 
Zhu et al. [31] 

The proposed  

The proposed 

2017 
2017 
2017 
2017 
2017 
2018 
2018 

2019 

2019 

Image 
Format 

Grayscale 
Grayscale 
Grayscale 
Grayscale 
Grayscale 
Grayscale 
Grayscale 

Method 

BoW 
LBP 
CNN 
VGG-16 
Inception-v3 
BoW 
MBoW 

Grayscale 

DenseNet-161 

RGB 

ResNet-50 

ğœ¼ ğ’‘ 

(%) 

64.98 
66.11 
64.98 
65.21 
74.87 
88.07 
89.21 

97.89 

98.87 

ğœ¼ ğ’˜ 

(%) 

61.02 
62.52 
64.75 
64.96 
76.10 
84.50 
85.30 

97.86 

98.89 

ğœ¼ ğ’•ğ’ğ’•ğ’‚ğ’ 
(%) 

39.65 
41.33 
42.07 
42.36 
56.98 
74.41 
76.09 

95.79 

97.77 

In 2017, Babaie et al. [22], employed the bag of words (BoW), local binary pattern (LBP) 

histograms, and convolutional neural networks (CNN) approaches to classify grayscale images. 

The total accuracy value ( ğœ¼ ğ’•ğ’ğ’•ğ’‚ğ’ ) for BoW, LBP and CNN approaches were reported as 39.65%, 

41.33%, and 42.07%, respectively. The highest classification accuracy, 42.07% was achieved with 

a CNN model that used 40,513 patches for the training of the network. The same year, Kieffer et 

al. [30], used data augmentation and transfer learning techniques to train VGG-16 and Inception-

v3 architectures. The Inception-v3 pre-trained CNN model achieved the highest total accuracy of 

56.98%. In 2018, Zhu et al. [31] proposed a BoW and a variant of BoW with multiple dictionaries 

(MBoW) approaches to classify histopathology images. The authors excluded patches that has a 

bright background and used the rest of 25,390 images for the training set. The Bow and MBoW 

methods reached the total accuracy of 74.41% and 76.09%, respectively.  

It can be seen from Table 4 that the proposed DenseNet-161 pre-trained CNN model outperforms 

other methods in all performance metrics for the classification of grayscale images. While the 

DenseNet-161 model has achieved a total accuracy of 95.79% for grayscale images, the ResNet-

50 model obtains a total accuracy of 97.77% in color images.  

13 

 
 
 
 
The main advantage of this study is that the proposed pre-trained CNN models have a better 

classification accuracy than other studies in the literature on the Kimia Path24 data set. In addition, 

the proposed method does not include any hand-made feature extraction technique, i.e., it has a 

fully automatic end-to-end structure. 

5. Conclusion 

The detection and classification of diseases from tissue samples are performed by pathologists 

manually. The diagnostic process by an expert using a microscope is time-consuming and 

expensive. Hence, the automated analysis of tissue samples from histopathology images has critical 

importance in terms of early diagnosis and treatment. It can also improve the quality of diagnoses 

and assist the pathologist for the decision making about the critical cases. In this study, the 

DenseNet-161 and ResNet-50 pre-trained models are employed to classify digital histopathology 

images into 24 categories. The classification performance of pre-trained models is compared for 

the color and grayscale images. The proposed DenseNet-161 model tested on grayscale images and 

obtained the best classification accuracy of 97.89% which outperforms the state-of-art methods. 

Additionally, the proposed ResNet-50 pre-trained model is tested on color images of Kimia Path24 

dataset and achieved a classification accuracy of 98.87%.  For future work, an ensemble learning 

method could be used to increase the classification performance.  

Conflict of interest 

There is no conflict of interest. 

References 

[1] Bhargava, R., & Madabhushi, A. (2016). Emerging themes in image informatics and molecular analysis 
for digital pathology. Annual review of biomedical engineering, 18, 387-412. 

[2] Stathonikos, N., Veta, M., Huisman, A., & van Diest, P. J. (2013). Going fully digital: Perspective of a 
Dutch academic pathology lab. Journal of pathology informatics, 4. 

14 

 
 
 
 
 
 
 
 
 
 
 
 
[3] Xu, J., Zhou, C., Lang, B. and Liu, Q., 2017. Deep learning for histopathological image analysis: 
Towards computerized diagnosis on cancers. In Deep Learning and Convolutional Neural Networks for 
Medical Image Computing (pp. 73-95). Springer, Cham. 

[4] Robbins, P., Pinder, S., De Klerk, N., Dawkins, H., Harvey, J., Sterrett, G., Ellis, I. and Elston, C., 1995. 
Histological grading of breast carcinomas: a study of interobserver agreement. Human pathology, 26(8), 
pp.873-879. 

[5] Rodenburg, B. (2016). Deep Learning in Histopathology Research Paper Business Analytics. 

[6] YÄ±ldÄ±rÄ±m, Ã–., San Tan, R., & Acharya, U. R. (2018). An efficient compression of ECG signals using 
deep convolutional autoencoders. Cognitive Systems Research, 52, 198-211. 

[7] YÄ±ldÄ±rÄ±m, Ã–., PÅ‚awiak, P., Tan, R. S., & Acharya, U. R. (2018). Arrhythmia detection using deep 
convolutional neural network with long duration ECG signals. Computers in biology and medicine, 102, 
411-420. 

[8] Oh, S. L., Ng, E. Y., San Tan, R., & Acharya, U. R. (2019). Automated beat-wise arrhythmia diagnosis 
using modified U-net on extended electrocardiographic recordings with heterogeneous arrhythmia 
types. Computers in biology and medicine, 105, 92-101. 

[9] KsiÄ…Å¼ek, W., Abdar, M., Acharya, U. R., & PÅ‚awiak, P. (2019). A novel machine learning approach for 
early detection of hepatocellular carcinoma patients. Cognitive Systems Research, 54, 116-127. 

[10] Baloglu, U. B., Talo, M., Yildirim, O., San Tan, R., & Acharya, U. R. (2019). Classification of 
Myocardial Infarction with Multi-Lead ECG Signals and Deep CNN. Pattern Recognition Letters. 

[11] Talo, M., Baloglu, U. B., YÄ±ldÄ±rÄ±m, Ã–., & Acharya, U. R. (2019). Application of deep transfer learning 
for automated brain abnormality classification using MR images. Cognitive Systems Research, 54, 176-
188. 

[12] LeCun, Y., Bengio, Y., & Hinton, G. (2015). Deep learning. nature, 521(7553), 436. 

[13] CireÅŸan, D. C., Giusti, A., Gambardella, L. M., & Schmidhuber, J. (2013, September). Mitosis 
detection in breast cancer histology images with deep neural networks. In International Conference on 
Medical Image Computing and Computer-assisted Intervention (pp. 411-418). Springer, Berlin, 
Heidelberg. 

[14] Cruz-Roa, A., Basavanhally, A., GonzÃ¡lez, F., Gilmore, H., Feldman, M., Ganesan, S., ... & 
Madabhushi, A. (2014, March). Automatic detection of invasive ductal carcinoma in whole slide images 
with convolutional neural networks. In Medical Imaging 2014: Digital Pathology (Vol. 9041, p. 904103). 
International Society for Optics and Photonics. 

[15] Maqlin, P., Thamburaj, R., Mammen, J. J., & Manipadam, M. T. (2015, December). Automated 
nuclear pleomorphism scoring in breast cancer histopathology images using deep neural networks. 
In International Conference on Mining Intelligence and Knowledge Exploration (pp. 269-276). Springer, 
Cham. 

[16] SzczypiÅ„ski, P., Klepaczko, A., Pazurek, M., & Daniel, P. (2014). Texture and color based image 
segmentation and pathology detection in capsule endoscopy videos. Computer methods and programs in 
biomedicine, 113(1), 396-411. 

15 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
[17] Saha, M., Chakraborty, C., & Racoceanu, D. (2018). Efficient deep learning model for mitosis 
detection using breast histopathology images. Computerized Medical Imaging and Graphics, 64, 29-40. 

[18] Han, Z., Wei, B., Zheng, Y., Yin, Y., Li, K., & Li, S. (2017). Breast cancer multi -classification from 
histopathological images with structured deep learning model. Scientific reports, 7(1), 4172. 

[19] Zheng, Y., Jiang, Z., Xie, F., Zhang, H., Ma, Y., Shi, H., & Zhao, Y. (2017). Feature extraction from 
histopathological images based on nucleus-guided convolutional neural network for breast lesion 
classification. Pattern Recognition, 71, 14-25. 

[20] Jia, Z., Huang, X., Eric, I., Chang, C., & Xu, Y. (2017). Constrained deep weak supervision for 
histopathology image segmentation. IEEE transactions on medical imaging, 36(11), 2376-2388. 

[21] Komura, D., & Ishikawa, S. (2018). Machine learning methods for histopathological image 
analysis. Computational and structural biotechnology journal, 16, 34-42. 

[22] Babaie, M., Kalra, S., Sriram, A., Mitcheltree, C., Zhu, S., Khatami, A., ... & Tizhoosh, H. R. (2017). 
Classification and retrieval of digital pathology scans: A new dataset. In Proceedings of the IEEE 
Conference on Computer Vision and Pattern Recognition Workshops (pp. 8-16). 

[23] Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). Imagenet classification with deep convolutional 
neural networks. In Advances in neural information processing systems (pp. 1097-1105). 

[24] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image 
recognition. arXiv: Comp. Res. Repository, abs/1409.1556, 2014 

[25] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proc. 
IEEE Conf. Comp. Vis. Patt. Recogn., 2016. 

[26] Huang, G., Liu, Z., Van Der Maaten, L., & Weinberger, K. Q. (2017). Densely connected convolutional 
networks. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 4700-
4708). 

[27] Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., ... & Lerer, A. (2017). Automatic 
differentiation in pytorch. 

[28] Howard, J., Others: The fast.ai deep learning library, github (2018) 

[29] http://kimia.uwaterloo.ca/kimia_lab_data_Path24.html, Accessed date: 3 March 2019. 

[30] Kieffer, B., Babaie, M., Kalra, S., & Tizhoosh, H. R. (2017, November). Convolutional neural networks 
for histopathology image classification: Training vs. using pre -trained networks. In 2017 Seventh 
International Conference on Image Processing Theory, Tools and Applications (IPTA) (pp. 1-6). IEEE. 

[31] Zhu, S., Li, Y., Kalra, S., & Tizhoosh, H. R. (2018). Multiple disjoint dictionaries for representation of 
histopathology images. Journal of Visual Communication and Image Representation, 55, 243-252. 

16 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
