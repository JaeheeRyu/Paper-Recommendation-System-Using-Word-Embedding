Towards Safety Veriï¬cation of Direct Perception
Neural Networks

Chih-Hong Chengâˆ— , Chung-Hao Huangâ€  , Thomas Brunnerâ€  and Vahid Hashemiâ€¡

âˆ—DENSO AUTOMOTIVE Deutschland GmbH
â€  fortiss - Research Institute of the Free State of Bavaria
â€¡Audi AG

Contact: c.cheng@denso-auto.de

â€œroad strongly bends to the rightâ€ and represent them as

5

1 In training neural networks, the value for each pixel in an image is
commonly re-scaled such that the re-scaled value is in the interval [0, 1].
Starting veriï¬cation using an input domain of [0, 1]dl0 with dl0 being the
number of input image pixels, the result of formal veriï¬cation always creates
counter-examples in formal veriï¬cation where counter-example images are
so distant from what can be observed in practice (such as images without
textures) and are rejected by experts.

9
1
0
2

v
o

N

1
2

]

E

S

.

s

c

[

2
v
6
0
7
4
0

.

4
0
9
1

:

v

i

X

r

a

Abstractâ€”We study the problem of safety veriï¬cation of direct
perception neural networks, where camera images are used as
inputs to produce high-level features for autonomous vehicles to
make control decisions. Formal veriï¬cation of direct perception
neural networks is extremely challenging, as it is difï¬cult to
formulate the speciï¬cation that requires characterizing input as
constraints, while the number of neurons in such a network
can reach millions. We approach the speciï¬cation problem by
learning an input property characterizer which carefully extends
a direct perception neural network at close-to-output layers, and
address the scalability problem by a novel assume-guarantee
based veriï¬cation approach. The presented workï¬‚ow is used to
understand a direct perception neural network (developed by
Audi) which computes the next waypoint and orientation for
autonomous vehicles to follow.
Index Termsâ€”formal veriï¬cation, neural network, dependabil-
ity, autonomous driving

I . IN TRODUC T ION

Using deep neural networks has been the de facto choice
for developing visual object detection function in automated
driving. Nevertheless, in the autonomous driving workï¬‚ow,
the neural networks can also be used more extensively. An
example is direct perception [2]; one trains a neural network
to read high-dimensional inputs (such as images from camera
or point clouds from lidar) and produce low-dimensional
information called affordances (e.g., safe maneuver regions or
the next waypoint to follow) which could be used to program
a controller for the autonomous vehicle. One may use direct
perception as a hot standby system for a classical mediated
perception system that extracts objects and identiï¬es lane
markings before affordances are produced.
In this paper, we study the safety veriï¬cation problem for a
neural network implementing direct perception, where the goal
is to ensure that under certain input conditions, the undesired
output values never occur. An example of such a kind can

be the following: â€œFor every input image where the road in
the image strongly bends to the right, the output of the neural
network should never suggest to strongly steer to the leftâ€.

Overall, the safety veriï¬cation problem for direct perception
networks is fundamentally challenging due to two factors:
â€¢ (Speciï¬cation) To perform safety veriï¬cation, one
premise is to have the undesired property formally
speciï¬ed. Nevertheless,
it
is practically impossible to
characterize input speciï¬cations from images such as

n17
1 , n17
2 , n17
3 , n17
4

constraints over input variables.
â€¢ (Scalability) Neural networks for direct perception often
take images with millions of pixels, and the internal
structure of the network can have many layers. This
challenges any state-of-the-art formal analysis framework
in terms of scalability.
Towards these issues, we present a workï¬‚ow for safety veri-
ï¬cation of direct perception neural networks by simultaneously
addressing the speciï¬cation and the scalability problem. For
the ease of understanding, we use Figure 1 to explain the con-
cept. First, we address the speciï¬cation problem by learning
an input property characterizer network, where the input of
the network is connected to close-to-output layer neurons of
the original direct perception network. In Figure 1, the input
property characterizer takes output values from the neurons
and n17
in the original deep perception
network. For the previously mentioned speciï¬cation, the input
property characterizer outputs true if an input image has â€œroad
strongly bending to the rightâ€. By doing so, the characterization
of input features is aggregated to an output of a neural network.
Subsequently, the safety veriï¬cation problem is approached by
asking if it is possible for the input-characterizing network to
output true, but the output of the direct perception network
demonstrates undesired values. As both the deep perception
network and the input-characterizing network have shared
neuron values, safety veriï¬cation can be approached by only
verifying close-to-output layers without losing soundness. In
Figure 1, safety veriï¬cation only analyzes the sub-network
colored grayed, and examines if any assignment of n17
2 ,
4 , and n17
leads to undesired output. The bounds
of the neurons n17
4 , and n17
can be decided
by static analysis (which guarantees an overly conservative
bound). However, using such a bound allows to have input
images that are not possible to be seen in the operating
design domain (ODD)1 . Thus we are advocating an alternative

5
1 , n17
2 , n17
3 , n17

n17
3 , n17

1 , n17

5

 
 
 
 
 
 
Fig. 1. High-level illustration how to perform safety veriï¬cation while tackling speciï¬cation and scalability issues.

assume-guarantee based approach where one ï¬rst creates an
outer polyhedron by aggregating all visited neuron values
computed by the training set. We use the created polyhedron
as a starting point to perform formal veriï¬cation, by assuming
that for every possible input data in the ODD, the computed
neuron activation pattern is contained in this polyhedron.
The assumption thus requires to be monitored in runtime
by checking if any computed neuron value falls outside the
polyhedron. As an example, we consider the bound of n17
1 to
be used in veriï¬cation in Figure 1. By observing the minimum
and the maximum of all visited values {0, 0.1, âˆ’0.1, . . . , 0.6},
[âˆ’0.1, 0.6] is an over-approximation over all visited values,
and one shall monitor in runtime whether the computed value
1 has fallen outside [âˆ’0.1, 0.6].2
The rest of the paper is organized as follows. Section II
presents the required deï¬nitions as well as the workï¬‚ow for
veriï¬cation. Section III discusses extensions to a statistical
setup when the input property characterizer is not perfect.
Lastly, we summarize related work in Section IV and conclude
with our preliminary evaluation in Section V.

of n17

I I . V ER I FICAT ION WORK FLOW

A deep neural network is comprised of L layers where
operationally, the l-th layer for l âˆˆ {1, . . . , L} of the net-
work is a function g (l) : Rdlâˆ’1 â†’ Rdl , with dl being the
dimension of layer l. Given an input in âˆˆ Rd0 , the output

2Note that such a monitoring is needed regardless of formal veriï¬cation,
as neurons at close-to-output
layers represent high-level features, so an
image in operation that leads to unexpectedly high or low neuron feature
intensity (indicated by falling outside the monitored interval) can be hints for
incomplete data collection or indicators for the system stepping out from the
ODD.

of the l-th layer of the neural network f (l) is given by the
functional composition of the l-th layer and the previous layers

f (l) (in) := â—¦(l)
i=1 g (i) (in) = g (l) (g (lâˆ’1) . . . g (2) (g (1) (in))).

A. Characterizing Input Speciï¬cation from Examples
Let InÏ† âŠ† Rd0 be the set of inputs of a neural network
that satisï¬es the property Ï†. We assume that both Ï† and InÏ†
are unknown (e.g., the road is bending left in an image), but
there exists an oracle (e.g., human) that can answer for a given
input in âˆˆ Rd0 , whether in âˆˆ InÏ† .
Let (In, CÏ† ) be the list of training data and their associated
labels (generated by the oracle) related to the input property Ï†,
where for every (in, c) âˆˆ (In, CÏ† ), in âˆˆ Rd0 , c âˆˆ {0, 1}, we

have (in, 1) âˆˆ (In, CÏ† ) if in âˆˆ InÏ† and (in, 0) âˆˆ (In, CÏ† ) if in (cid:54)âˆˆ

(In, CÏ† ), hÏ†

InÏ† . The perfect input property characterizer extending the l-th
l which guarantees that for every (in, c) âˆˆ
layer is a function hÏ†
l (f (l) (in)) = c. The generation of hÏ†
l can be done
by training a neural network as a binary classiï¬er, with 100%
success rate on the training data. The following assumption
states that as long as function hÏ†
l performs perfectly on the
training data, hÏ†
l will also perfectly generalize to the complete
input space. In other words, we can use hÏ†
to characterize Ï†.
Assumption 1 (Perfect Generalization). Assume that hÏ†
l also
perfectly characterizes Ï†, i.e., âˆ€in âˆˆ Rd0 : hÏ†

l

l (f (l) (in)) = 1 iff

in âˆˆ InÏ† .

Deï¬nition 1 (Safety Veriï¬cation). The safety veriï¬cation
problem asks if there exists an input in âˆˆ InÏ† such that f (L) (in)
satisï¬es Ïˆ , where the risk condition Ïˆ is a conjunction of
linear inequalities over the output of the neural network. If
no such input in exists, we say that the neural network is safe
under the input constraint Ï† and the output risk constraint Ïˆ .

InputOutput (vehicle direction)Deep layers with convolutionğ‘›117ğ‘›217ğ‘›317ğ‘›417ğ‘›517Image1{0, 0.1, -0.1, â€¦., 0.6}Abstraction: [-0.1, 0.6]{-0.3, 0.2, 1.1, â€¦., 0.8}Abstraction: [-0.3, 1.1]ğ‘›319ğ‘›219ğ‘›119ğ‘›419ğ‘›519Input property characterizer networkCharacterizing â€œroad bendingâ€ conditionsLayers and neurons that are considered in safety verification(from ODD)When Assumption 1 holds, for safety veriï¬cation it
is
equivalent to ask whether there exists an input in âˆˆ Rd0 such
that hÏ†
l (f (l) (in)) = 1 and f (L) (in) satisï¬es Ïˆ . From now on,
unless explicitly speciï¬ed, we consider only situations where
Assumption 1 holds.

B. Practical Safety Veriï¬cation
a) Abstraction by omitting neurons before the l-th layer.:
The following result states that one can retain soundness for
safety veriï¬cation, by considering all possible neuron values
that can appear in the l-th layer.
Lemma 1 (Veriï¬cation by Layer Abstraction).

exists no Ë†nl âˆˆ Rdl such that g (L) (g (Lâˆ’1) . . . (g (l+1) ( Ë†nl ))

there

If

satisï¬es Ïˆout and hÏ†
l ( Ë†nl ) = 1, then the neural network is
safe under input constraint Ï† and output risk constraint Ïˆ .
Proof. The lemma holds because for every input in âˆˆ Rd0 of
the network, f (l) (in) âˆˆ Rdl .
Obviously, the use of Rdl in Lemma 1 is overly conservative,
and we can strengthen Lemma 1 without losing soundness,
if we ï¬nd S âŠ† Rdl which guarantees that f (l) (in) âˆˆ S for
every input in âˆˆ Rd0 of the network. Obtaining such a set S
can be achieved by abstract interpretation techniques [6], [21]
which perform symbolic reasoning over the neural network in
a layer-wise manner.
Lemma 2 (Abstraction via Input Over-approximation). Let
S âŠ† Rdl guarantee that f (l) (in) âˆˆ S for every input in âˆˆ
Rd0 of the network. If there exists no Ë†nl âˆˆ S such that

g (L) (g (Lâˆ’1) . . . (g (l+1) ( Ë†nl )) satisï¬es Ïˆout and hÏ†
l ( Ë†nl ) = 1,

then the neural network is safe under input constraint Ï† and
output risk constraint Ïˆ .

b) Assume-guarantee Veriï¬cation via Monitoring.: If the
computed S , due to over-approximation, is too coarse to prove
safety, one practical alternative is to generate ËœS which only
guarantees f (l) (in) âˆˆ ËœS for every input in âˆˆ In in the training
data. In other words, ËœS over-approximates the neuron values
computed based on the samples in the training data.
If using ËœS is sufï¬cient to prove safety and if for any input
in, checking whether f (l) (in) âˆˆ ËœS can be computed efï¬ciently,
one can conditionally accept the proof by designing a run-
time monitor which raises a warning that
the assumption
f (l) (in) âˆˆ ËœS used in the proof is violated. Admittedly, ËœS can
be an under-approximation over {f (l) (in) | in âˆˆ Rd0 }, but
practically creating an over-approximation only based on the
training data is useful and can avoid unstructured input such
as noise which is allowed when using Rd0 .

I I I . TOWARD S S TAT I ST ICAL R EA SON ING

The results in Section II are based on two assumptions of
perfection, namely
â€¢ (perfect training) the input property characterizer per-
fectly decides whether property Ï† holds, for each sample
in the training data, and

hÏ†
l (f (l) (in)) = 1
hÏ†
l (f (l) (in)) = 0

in âˆˆ InÏ†

Î±
Î³

in (cid:54)âˆˆ InÏ†

Î²
1 âˆ’ Î± âˆ’ Î² âˆ’ Î³

TABLE I

PROBAB I L I TY BY CON S ID ER ING A LL PO S S IB LE CA SE S DUE TO DEC I S ION S
MAD E BY TH E IN PU T CHARACT ER I Z ER (WH ETH ER hÏ†
l (f (l) ( IN)) = 1 )
AND TH E GROUND TRU TH (WH ETH ER IN âˆˆ INÏ† ) .

â€¢ (perfect generalization) the input property characterizer
generalizes its decision (whether property Ï† holds) also
perfectly to every data point in the complete input space.
One important question appears when the above two as-
sumptions do not hold, meaning that it is possible for the
input property characterizer to make mistakes. By considering
all four possibilities in Table I, one realizes that even when
a safety proof is established by considering all inputs where
l (f (l) (in)) = 1, there exists a probability Î³ where an input in
should be analyzed, but in is omitted in the proof process due
to hÏ†
Therefore, one can only establish a statistical guarantee with
(1 âˆ’ Î³ ) probability over the correctness claim3 , provided that
all data points used in training hÏ†
l are also safe4 .

l (f (l) (in)) being 0 (i.e., in âˆˆ InÏ† and hÏ†

l (f (l) (in)) = 0).

hÏ†

IV. R E LATED WORK

Formal veriï¬cation of neural networks has drawn huge
attention with many results available [13], [8], [3], [5], [9],
[11], [6], [4], [1], [14], [20], [19], [21]. Although speciï¬cations
used in formal veriï¬cation of neural networks are discussed
in recent reviews [7], [15], the speciï¬cation problem in terms
of characterising an image set is not addressed, so research
results largely use inherent properties of a neural network such
as local robustness (as output invariance) or output ranges
where one does not need to characterize properties over a
set of input images. Not being able to properly characterizing
input conditions (one possibility is to simply consider every
input to be bounded by [âˆ’1, 1]) makes it difï¬cult for formal
static analysis to achieve any useful results on deep perception
networks, regardless of the type of abstraction domain being
used (box, octagon, or zonotope). Lastly, our work is motivated
by zero shot learning [12] which trains additional features
apart from a standard neural network. The feature detector is
commonly created by extending the network from close-to-
output layers.

V. EVALUAT ION AND CONC LUD ING R EMARK S

We have applied this methodology to examine a direct
perception neural network developed by Audi. The network
acts as a hot standby system and computes the next waypoint

3Note that for parts where in (cid:54)âˆˆ InÏ† and hÏ†
problem occurs as the safety analysis guarantees the desired property when
4 In other words, for every (in, c) âˆˆ (In, CÏ† ), if hÏ†
c = 1, then f (L) (in) does not satisfy Ïˆ .

l (f (l) (in)) = 0, no

l (f (l) (in)) = 0 and

hÏ†
l (f (l) (in)) = 1.

and orientation for autonomous vehicles to follow. As the
close-to-output
layers of the network are either ReLU or
Batch Normalization, and as Ïˆ is a conjunction of linear
constraints over output, it is feasible to use exact veriï¬cation
methods such as ReLUplex [8], Planet [5] or MILP-based
approaches [3], [9] as the underlying veriï¬cation method. We
developed a variation of nn-dependability-kit5 to read models
from TensorFlow6 and to perform formal veriï¬cation via a
reduction to MILP. Using assume-guarantee based techniques
that take an over-approximation from neuron values produced
by the training data7 , it is possible to conditionally prove some
properties such as â€œimpossibility to suggest steering to the far
left, when the road image is bending to the rightâ€. However,
under the current setup, it is still impossible to prove intriguing
properties such as â€œimpossibility to suggest steering straight,
when the road image is bending to the rightâ€. We suspect that
the main reason is due to the inherent limitation of the neural
network under analysis.
In our experiment, we also found that for some input
properties such as trafï¬c participants in adjacent lanes, it is
very difï¬cult to construct the corresponding input property
characterizers by taking neuron values from close-to-output
layers (i.e., the trained classiï¬er almost acts like fair coin
ï¬‚ipping). Based on the theory of information bottleneck for
neural networks [18], [16], a neural network from high di-
mensional input to low dimensional output naturally eliminates
unrelated information in close-to-output layers. Therefore, the
input property can be unrelated to the output of the network.
Although we are unable to prove that the output of the network
is safe under these input constraints, it should be possible to
construct a counter example either by capturing more data or
by using adversarial perturbation techniques [17], [10].
To achieve meaningful formal veriï¬cation, in our experi-
ments, we also realized that it is commonly not sufï¬cient to
only record the minimum and maximum value for each neuron,
as boxed abstraction can lead to huge over-approximation. In
certain circumstances, we also record the minimum and max-
imum difference between two adjacent neurons in a layer (in
i where i âˆˆ {1, 2, 3, 4}). Modern
Figure 1, we record n17
training frameworks such as TensorFlow support computing
differences of adjacent neurons with GPU parallelization8 ,
thereby making monitoring possible.
Overall, our initial result demonstrates the potential of using
formal methods even on very complex neural networks, while
it provides a clear path to engineers to resolve the problem
related to how to characterize input conditions for veriï¬cation
(by also applying machine learning techniques). Our approach
of looking at close-to-output
layers can be viewed as an
abstraction which can, in future work, leads to layer-wise

i+1 âˆ’n17

5 https://github.com/dependable- ai/nn- dependability- kit/
6 https://www.tensorï¬‚ow.org/
7 The data is taken from a particular segment of the German A9 highway,
by considering variations such as weather and the current lane.
8Computing the neuron difference, when neuron values are stored in an 1D
tensor n can be done in numpy using a single instruction diff(n), and in
TensorFlow using n[1 :] âˆ’ n[: âˆ’1].

incremental abstraction-reï¬nement techniques. Although our
practical motivation is to verify direct perception networks, the
presented technique is equally applicable to any deep network
for vision and lidar systems where input constraints are hard
to characterize. It opens a new research direction of using
learning to assist practical veriï¬cation of learning systems.
Acknowledgement The research work is conducted during
the ï¬rst authorâ€™s service at the fortiss research institute and
is supported by the following projects: â€œAudi Veriï¬able AI â€
from Audi AG, Germany and â€œDependable AI for automotive
systemsâ€ from DENSO Corporation, Japan.

R E FER ENC E S
[1] R. R. Bunel, I. Turkaslan, P. Torr, P. Kohli, and P. K. Mudigonda. A
uniï¬ed view of piecewise linear neural network veriï¬cation. In NIPS,
pages 4795â€“4804, 2018.
[2] C. Chen, A. Seff, A. Kornhauser, and J. Xiao. Deepdriving: Learning
affordance for direct perception in autonomous driving. In ICCV, pages
2722â€“2730. IEEE, 2015.
[3] C.-H. Cheng, G. N Â¨uhrenberg, and H. Ruess. Maximum resilience of
artiï¬cial neural networks. In ATVA, pages 251â€“268. Springer, 2017.
[4] S. Dutta, S. Jha, S. Sankaranarayanan, and A. Tiwari. Output range
analysis for deep feedforward neural networks.
In NFM, pages 121â€“
138. Springer, 2018.
[5] R. Ehlers. Formal veriï¬cation of piece-wise linear feed-forward neural
networks. In ATVA, pages 269â€“286. Springer, 2017.
[6] T. Gehr, M. Mirman, D. Drachsler-Cohen, P. Tsankov, S. Chaudhuri, and
M. Vechev. Ai2: Safety and robustness certiï¬cation of neural networks
with abstract interpretation. In Oakland, pages 3â€“18. IEEE, 2018.
[7] X. Huang, D. Kroening, M. Kwiatkowska, W. Ruan, Y. Sun, E. Thamo,
M. Wu, and X. Yi. Safety and trustworthiness of deep neural networks:
A survey. arXiv:1812.08342, 2018.
[8] G. Katz, C. W. Barrett, D. L. Dill, K. Julian, and M. J. Kochenderfer.
Reluplex: An efï¬cient SMT solver for verifying deep neural networks.
In CAV, pages 97â€“117, 2017.
[9] A. Lomuscio and L. Maganti. An approach to reachability analysis for
feed-forward ReLU neural networks. arXiv:1706.07351, 2017.
[10] S.-M. Moosavi-Dezfooli, A. Fawzi, O. Fawzi, and P. Frossard. Universal
adversarial perturbations.
In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pages 1765â€“1773, 2017.
[11] N. Narodytska, S. Kasiviswanathan, L. Ryzhyk, M. Sagiv, and T. Walsh.
Verifying properties of binarized deep neural networks. In AAAI, pages
6615â€“6624, 2018.
[12] M. Palatucci, D. Pomerleau, G. E. Hinton, and T. M. Mitchell. Zero-shot
learning with semantic output codes. In NIPS, pages 1410â€“1418, 2009.
[13] L. Pulina and A. Tacchella. An abstraction-reï¬nement approach to
veriï¬cation of artiï¬cial neural networks.
In CAV, pages 243â€“257.
Springer, 2010.
[14] W. Ruan, X. Huang, and M. Kwiatkowska. Reachability analysis of deep
neural networks with provable guarantees. In IJCAI, pages 2651â€“2659,
2018.
[15] S. A. Seshia, A. Desai, T. Dreossi, D. J. Fremont, S. Ghosh, E. Kim,
S. Shivakumar, M. Vazquez-Chanlatte, and X. Yue. Formal speciï¬cation
for deep neural networks. In ATVA, pages 20â€“34. Springer, 2018.
[16] R. Shwartz-Ziv and N. Tishby. Opening the black box of deep neural
networks via information. arXiv:1703.00810, 2017.
[17] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Good-
fellow, and R. Fergus.
Intriguing properties of neural networks.
arXiv:1312.6199, 2013.
[18] N. Tishby and N. Zaslavsky. Deep learning and the information
bottleneck principle. In ITW, pages 1â€“5. IEEE, 2015.
[19] S. Wang, K. Pei, J. Whitehouse, J. Yang, and S. Jana. Formal security
analysis of neural networks using symbolic intervals. In USENIX, pages
1599â€“1614, 2018.
[20] T.-W. Weng, H. Zhang, H. Chen, Z. Song, C.-J. Hsieh, D. Boning, I. S.
Dhillon, and L. Daniel. Towards fast computation of certiï¬ed robustness
for ReLU networks. In ICML, pages 5276â€“5285, 2018.
[21] P. Yang, J. Liu, J. Li, L. Chen, and X. Huang. Analyzing deep neural
networks with symbolic propagation: Towards higher precision and
faster veriï¬cation. arXiv:1902.09866, 2019.

