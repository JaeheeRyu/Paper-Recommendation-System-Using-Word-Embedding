9
1
0
2

v
o

N

1
2

]
I

S

.

s

c

[

2
v
2
0
4
8
0

.

8
0
9
1

:

v

i

X

r

a

Temporal Neighbourhood Aggregation: Predicting
Future Links in Temporal Graphs via Recurrent
Variational Graph Convolutions

Stephen Bonner â€  , Amir Atapour-Abarghouei â€¡ , Philip T Jackson â€  , John Brennan â€  , Ibad Kureshi Â¶ ,
Georgios Theodoropoulos Â§ , Andrew Stephen McGough â€¡ and Boguslaw Obara â€ 

â€ Department of Computer Science, Durham University, Durham, UK,
{s.a.r.bonner, p.t.g.jackson, j.d.brennan, boguslaw.obara}@durham.ac.uk
â€¡School of Computing, Newcastle University, Newcastle, UK, {amir.atapour-abarghouei, stephen.mcgough}@newcastle.ac.uk
Â§School of Computer Science and Engineering, SUSTech, Shenzhen, China, georgios@sustec.edu.cn
Â¶ Inlecom Systems, Brussels, Belgium,
ibad.kureshi@inlecomsystems.com

Abstractâ€”Graphs have become a crucial way to represent
large, complex and often temporal datasets across a wide range
of scientiï¬c disciplines. However, when graphs are used as input
to machine learning models, this rich temporal information is
frequently disregarded during the learning process, resulting in
suboptimal performance on certain temporal inference tasks. To
combat this, we introduce Temporal Neighbourhood Aggregation
(TNA), a novel vertex representation model architecture designed
to capture both topological and temporal information to directly
predict future graph states. Our model exploits hierarchical
recurrence at different depths within the graph to enable explo-
ration of changes in temporal neighbourhoods, whilst requiring
no additional features or labels to be present. The ï¬nal vertex
representations are created using variational sampling and are
optimised to directly predict the next graph in the sequence. Our
claims are supported by experimental evaluation on both real and
synthetic benchmark datasets, where our approach demonstrates
superior performance compared to competing methods, outper-
forming them at predicting new temporal edges by as much as
23% on real-world datasets, whilst also requiring fewer overall
model parameters.
Index Termsâ€”representation learning, dynamic link prediction

I . IN TRODUC T ION

Using graphs to represent relationships in large, complex
and high-dimensional datasets has become a universal phe-
nomenon across many scientiï¬c ï¬elds. Encompassing not only
computer scientists, interested in social and citation networks
[1], but biologists, studying protein interaction graphs for
associations with diseases [2], chemists, who model molecule
properties by treating them as graphs [3], and physicists, who
use graphs to model a physical environment [4].
Using graph-based approaches enables complex data analy-
sis, with one of the most universal being the identiï¬cation of
missing links within the graph, which can provide invaluable
insight in many real-world scenarios. For example, the recom-
mendation of acquaintances on social networks, new research
papers to read or even new links between molecules. However,
to date, almost all of the prediction work performed on graphs

v4

3

v2

3

v1

3

v3

3

v4

2

v2

2

v1

2

v3

2

v4

1

v2

1

v1

1

v3

1

G

T

G

2

G

1

Fig. 1: The temporal link prediction task is to predict the new
edges (red) in the ï¬nal graph snapshot GT (green plane) given
the previous graphs G1 and G2 .

has been focused on analysis in solely the topological domain,
ignoring the rich temporal information inherent in so much of
the data represented by graphs (as seen Figure 1).
We formally deï¬ne a graph G = (V , E ) as a ï¬nite set of
vertices V , with a corresponding set of edges E . Elements of
E are unordered tuples {i, j } where i, j âˆˆ V . Elements in V
and E may have labels or certain associated features, although
these are not required for this work. In order to perform
analysis on graphs, we need a mechanism which converts the
formal graph representation into a format which is amenable
for machine learning â€“ graph representation learning.
The ï¬eld of graph representation learning has received
signiï¬cant attention as a means of analysing large, complex
graphs via the use of machine learning. Graph representation
learning, comprises a set of techniques that learn latent rep-
resentations of a graph, which can then be used as the input
to machine learning models for downstream prediction tasks
[5]. The majority of graph representation learning techniques

 
 
 
 
 
 
have focused upon learning vertex embeddings [6] and re-
constructing missing edges [5]. As such, the goal of graph
representation learning is to learn some function f : V â†’ Rd
which maps from the set of vertices V to a set of embeddings
of the vertices, where d is the required dimensionality. This
results in f being a mapping from G to a representation
matrix of dimensions |V | Ã— d, i.e. an embedding of size d
for each vertex in the graph. However, the majority of graph
representation learning approaches to date ignore the temporal
aspect of dynamic graphs, resulting in models which perform
poorly at predicting future change in a graph.
This paper introduces a new model, entitled Temporal
Neighbourhood Aggregation (TNA), designed to learn vertex
representations which capture both topological and temporal
change by exploiting the rich information found in large
dynamic graphs. To achieve this, we propose a novel model
architecture combining graph convolutions with recurrent con-
nections on the resulting vertex level representations to al-
low for powerful, hierarchical learning at multiple hops of
a vertices neighbourhoods. This approach means the model
can explore at which neighbourhood depth the most useful
temporal information can be learned. Further, we aggregate
the temporal neighbourhood using tools from variational infer-
ence, resulting in a more robust and stable ï¬nal representation
for each vertex. Our TNA model is trained end to end on
temporal graphs represented as time snapshots, where the
objective is to directly and accurately predict the next graph
in the sequence using the embeddings alone. This results in
a model, which unlike many competing approaches, requires
no explicitly parameterized decoder model. In summary, our
primary contributions are as follows:
â€¢ Temporal Neighbourhood Aggregration - Our proposed
model is capable of independently learning the tempo-
ral evolutionary patterns within the neighbourhood of a
vertex at different depths, resulting in superior perfor-
mance at predicting future links. Moreover, our approach
requires no additional vertex features, labels or random
walk procedures as part of its process.
â€¢ Variational Sampling - More robust temporal representa-
tions and consequently accurate prediction of the next
graph in the evolving sequence is made possible by
our approach by sampling vertex embeddings using the
principals of variational inference.
â€¢ Model Efï¬cacy and Scalability - Our model contains sig-
niï¬cantly fewer parameters than competing approaches,
as it does not require a parameterized decoder portion.
This leads to our model being scalable to larger graphs
as a result of its memory efï¬ciency.

Our work is supported by extensive experimentation on
public benchmark datasets. Further, to aid reproducibility, we
open-source all of our PyTorch [7] based source-code1 and
experimentation scripts.

1 https://github.com/sbonner0/temporal- neighbourhood- aggregation

I I . R E LATED WORK S

We highlight prior work in the areas of graph representation
learning and temporal embeddings.

A. Graph Representation Learning
Historically, low dimensional graph representations were
created via matrix factorization techniques. Examples of such
approaches include Laplican eigenmaps [8] and Graph Factor-
ization [9]. More recent models, originally used for Natural
Language Processing (NLP) tasks, have been adapted to learn
graph embeddings. These approaches exploit random walks to
create â€˜sentencesâ€™ which can be used as input to language-
inspired models such as DeepWalk [10] and Node2Vec [5].
Graph-speciï¬c neural network based models have been
created, inspired by Convolutional Neural Networks (CNN).
Such approaches attempt to create a differential model for
learning directly from graph structures. Many Graph CNN
approaches operate in the spectral domain of the graph, using
eigenvectors derived from the Laplacian matrix of a graph
[1]. The Graph Convolutional Network (GCN) approach has
proven to be particularly effective [1]. GCN uses a layer-
wise propagation rule to aggregate information from the 1-hop
neighbourhood of a vertex to create its representation. This
layer-wise rule can be stacked k times to aggregate information
from k hops away.
The approaches discussed thus far have been supervised,
mandating the use of labels. However, graph embedding
approaches exist which are based on auto-encoders - a type
of neural network trained to reconstruct the input data after
initially being projected into a lower dimension [11]. For
example, GCNs have been used as the basis of a convolutional
auto-encoder model [12], demonstrating state-of-the-art results
for static link prediction.

B. Temporal Graph Embeddings
We argue that the existing approaches for temporal graph
embeddings can be split in two categories: Temporal Walk and
Adjacency Matrix Factorisation.
1) Temporal Walk Approaches:
In an approach entitled
STWalk [13], the authors aim to learn node trajectories via
the use of random walks which learn representations that
consider all the previous time-steps of a temporal graph. In
the best performing approach presented, the authors learn two
representations for a given vertex simultaneously which are
concatenated to create the ï¬nal temporal embedding. However,
the approach is not end to end and requires the user to
manually chose how many time steps to consider.
Yu et al. [14], propose NetWalk, which enables anomaly
detection in streaming graphs via a vertex-level dynamic graph
embedding model. In the approach, a collection of short
random walks captured from the graph is passed into an auto-
encoder based model to create the vertex representations.
Nguyen et al. [15], propose a model to incorporate temporal
information when creating graph embeddings via random
walks by capturing individual
temporal changes within a
graph. They propose a temporal random walk to create the

input data, with the approach producing more complex and
rich temporal walks via a biasing process.
2) Adjacency Matrix Factorisation Approaches: Goyal et
al. [16], propose a model for creating dynamic graph em-
beddings, entitled DynGEM. In this approach, they extend
the auto-encoder graph embedding model of Structural Deep
Network Embedding (SDNE) [17] to consider dynamic graphs,
by using a method similar to Net2net [18], which is designed
to transfer knowledge from one neural network to a second.
In a family of approaches entitled Dyngraph2vec*, com-
prised of DynAE, DynRNN and DynAERNN, Goyal et al. [19]
further extend an SDNE type approach to incorporate temporal
information in a variety of ways. The best performing of
approaches, DynAERNN, uses a combination of SDNE-like
dense auto-encoders, with stacked recurrent layers to learn
temporal information when creating vertex embeddings. How-
ever, they do not make use of graph convolutions and require
a complex decoder model to predict the next graph.
There have been attempts to incorporate temporal aspects
into GCNs. However, some [20], [21] focus upon supervised
learning, do not explicitly use the models to predict
the
future graph state and only have a single layer of recurrent
connections. More recent approaches, such as GCN-GAN
[22] and GC-LSTM [23] require large and complex decoder
models, meaning they cannot scale to graphs of one-thousand
vertices or more on current hardware, whilst also lacking
the variational sampling of our approach. In comparison,
EvolveGCN [24] uses recurrent layers to directly evolve the
parameters of standard GCN layers which means it does not
track vertex neighbourhood evolution explicitly.
One of the application areas most frequently learning tem-
poral models on graphs is that of trafï¬c modelling. Where
approaches like [25] and [26] combine graph learning with
temporal models to predict trafï¬c movement. However, unlike
these approaches we focus on creating vertex level embeddings
directly optimised to predict future edges and learn change at
different hops of a vertices neighbourhood.

I I I . M ETHODO LOGY

We brieï¬‚y outline the proposed approach, relevant back-
ground, network architecture and the training procedure.
Throughout, we make use of the notation in Table I.

A. Motivation
Many of the phenomena that are commonly represented via
graph structures are known to evolve over time â€“ Links be-
tween entities form and break in a constantly evolving stream
of changes. We thus view graphs as a series of snapshots, with
each graph snapshot containing the connections present at that
particular moment in time. More formally, we can redeï¬ne a
graph G to be a temporal graph G(cid:48) = {G1 , G2 , ..., GT }, where
each graph snapshot Gt âˆ€t âˆˆ [1, T ] contains a corresponding
vertex set Vt and edge set Et .
A common and vital task within the ï¬eld of graph mining
is that of future link prediction, where the goal is to accurately
predict which vertices within a graph will form a connection in

Symbol

Deï¬nition

G

A

Ë†A

X

H

Z

G(cid:48)

T
Gt
Ïƒs
Ïƒr

Ïƒlr

l

W (l)
g
W (l)
s
W (l)
{r,u,h}
U (l)
{r,u,h}

N (Âµ, Ïƒ)

Î˜

2 (A + I )Dâˆ’ 1

{G1 , G2 , ..., GT }.

A graph with an associated set of vertices V and corre-
sponding set of edges E .
The adjacency matrix of graph G, a symmetric matrix of
size |V | Ã— |V |, where (ai ,j ) is 1 if an edge is present
and 0 otherwise.
A normalised by its degree matrix D and its identity
matrix I such that Ë†A = (Dâˆ’ 1
2 ) [1].
A matrix of features for each v âˆˆ V , set to the identity
I of A for this work.
The intermediate vertex representations in GCN and TNA
layers.
The ï¬nal variationally sampled representation matrix for
each v âˆˆ V .
A
temporal
graph
comprised
The number of snapshots in G(cid:48) .
A graph from G(cid:48) .
The sigmoid activation function.
The rectiï¬ed linear activation function (ReLU).
The leaky ReLU activation function.
A certain layer in the model.
A weight matrix at layer l used in the GCN.
A weight matrix at layer l used in the skip connection.
Hidden transform matrices in the GRU.
Input transform matrices in the GRU.
A multi-dimensional Gaussian distribution parametrised
by vectors Âµ and Ïƒ .
A trainable model containing a set of parameters.

of

snapshots

TABLE I: Deï¬nitions and Notations

the future [16]. Figure 1 highlights this future link prediction
task, where the goal is to predict the new edges, coloured in
red, formed in GT , given the previous graphs in the temporal
history G1 and G2 . Any model designed to accomplish this
task must learn the evolution patterns present in edge forma-
tion, even though the number of edges changing at each time
point is often a small fraction of the total number.
We propose to tackle this by creating temporally-aware
graph embeddings, which are explicitly trained to recreate
a future time step of the graph. We entitle our approach
Temporal Neighbourhood Aggregation (TNA), since to create
a better and more meaningful representation for a certain
vertex, the model is able to aggregate information about how
its neighbourhood has changed in the past to more accurately
predict how it will change into the future. More concretely,
a temporal graph G(cid:48)
is input
to our TNA model Î˜(G(cid:48) )
which learns a representation for each vertex in Gt âˆˆ G(cid:48)
such that its output can accurately predict the graph Gt+1 .
Ideally, we want to create a model Î˜() which can perform this
temporal learning using just the sequence of graphs until Gt ,
such that Gt+1 = Î˜(G1 , ..., Gt ). TNA is able to accomplish
this, requiring no pre-processing steps which could affect the
models performance (e.g. random walk procedures), no pre-
computed vertex features and no additional labels.

B. Background Technologies
We ï¬rst review the background technologies we are em-
ploying to make it possible, namely Graph Convolutions [1]
and Recurrent Neural Networks [27], [28].

1) Graph Convolutions: To perform the graph encoding
required to create the initial vertex representations, we utilise
the spectral Graph Convolution Networks (GCN) [1]. One can
consider a GCN to be a differentiable function for aggregating
information from the immediate neighbourhood of vertices
[29], [30]. A GCN takes the normalised adjacency matrix Ë†A
representing a graph G, and a matrix of initial vertex level
features X , and computes a new matrix of vertex level features
H = GCN ( Ë†A, X ). X can be initialized with pre-computed
vertex features, but it is sufï¬cient to initialize with one-hot
feature vectors (in which case X is the identity matrix I ). A
GCN can contain many layers which aggregate the data, where
the operation performed at each layer by the GCN [1] is:

GCN (l) (H (l) , Ë†A) = Ïƒr ( Ë†AH (lâˆ’1)W (l)
g ) ,

(1)

g

where l is the number of the current layer, W (l)
denotes
the weight matrix of that layer, H (lâˆ’1) refers to the features
computed at the previous layer or is equal to X at l = 0.
One can consider the GCN function to be aggregating
a weighted average of the neighbourhood features for each
vertex in the graph. Stacking multiple GCN layers has the
effect of increasing the number of hops from which a vertex-
level representation can aggregate information â€“ a three layer
GCN will aggregate information from three-hops within the
graph to create each representation.
The original method requires GCN based models to be
trained in a supervised learning framework, where the ï¬nal
vertex representation is tuned via labels provided for a speciï¬c
task â€“ classiï¬cation being common [1], [30]. Extensions
to the GCN framework have been made which allow for
convolutional auto-encoders for graph datasets [12].
2) Recurrent Neural Networks (RNN): RNN are neural
networks with circular dependencies between neurons. Activa-
tions of a recurrent layer are dependent on their own previous
activations from a previous forward pass, and therefore form
a type of internal state that can store information across time
steps. They are frequently used in sequence processing tasks
where the response at one time step should depend in some
way on previous observations. Long Short-Term Memory
(LSTM) [27] and Gated Recurrent Units (GRU) [28] are
RNNs with learned gating mechanisms, which mitigate the
vanishing gradient problem when back-propagating errors over
a sequence of inputs, allowing the model to learn longer-term
dependencies. For this work, we employ the GRU cell, as it
empirically offers similar performance to an LSTM, but with
fewer overall parameters. The GRU computes the output ht ,
for the input vector xt at time t in the following manner [28]:

(cid:0)xtU (l)
(cid:0)xtU (l)
u + htâˆ’1W (l)
u
r + htâˆ’1W (l)
r

Ëœht = tanh (cid:0)xtU (l)
ut = Ïƒs
rt = Ïƒs
h + (rt âˆ— htâˆ’1 )W (l)
ht = (1 âˆ’ ut ) âˆ— htâˆ’1 + ut âˆ— Ëœht ,

(cid:1)
(cid:1)

(cid:1)

(2)

h

where r and u are the rest and update gates and Ïƒs and tanh
are the sigmoid and hyperbolic tangent activation functions.

Fig. 2: An overview of the Temporal Neighbourhood Aggre-
gation (TNA) block, which comprises a Graph Convolutional
Network (GCN) layer with a Gated Recurrent Unit (GRU).
The combination of the topological and temporal learning is
controlled via the ï¬nal linear layer.
C. Model Overview
We ï¬rst detail the Temporal Neighbourhood Aggregation
blocks which form the primary learning component, before
describing the overall model topology and objective function.
1) TNA Block: One of the primary components of our
model is the TNA block for topological and temporal learning
from graphs. The overall structure of the block is illustrated
in Figure 2. It is important to note that all the parameters
in the block are shared through time. This allows complex
temporal patterns to be learned, as well as allowing for a large
reduction in the total number of parameters required by the
model. Assuming that the TNA block is the ï¬rst layer in the
model, the ï¬‚ow for vertex v âˆˆ Vt can be described as follows:
â€¢ The input is passed through the GCN layer, as detailed
in Equation 1, which will learn to aggregate information
for v from its one-hop neighbourhood to create its repre-
sentation at this point in the block - hGCN
. This is then
normalised using Layer Norm [31], which will ensure that
the representation for each vertex is of a similar scale,
this has been shown to improve the training stability and
convergence rate of deep models [31].
â€¢ This normalised representation is then passed into a GRU
cell a row at a time, as detailed in Equation 2, where the
output of the cell will be a function of the current input
as well as all the previous inputs. Meaning that the cell
can learn how much of the previous neighbourhood rep-

t

GCNLayerNormGRUConcatLinearInputTopology LearningTemporal LearningLayerNormSkip ConnectionOutputFig. 3: The overall Temporal Neighbourhood Aggregation Model: two stacked TNA blocks learning both topological and
temporal information from the ï¬rst and second hop neighbourhoods of a vertex. An embedding zt is sampled for each vertex
vt âˆˆ Vt using variational inference. The inner product is then used to directly predict the next graph in the sequence.

t

t

t

and hGRU

resentation to use when creating the new representation
for a given vertex hGRU
. This is then passed through a
second Layer Norm unit to ensure a normalised output.
â€¢ Finally, the hGCN
representations are concate-
nated together, before being passed through a linear layer
and a leaky ReLU activation function to create the ï¬nal
representation for the vertex hT N A
. Inspired by residual
connections often used in computer vision networks [32],
this enables the model
to learn the optimum mix of
topological and temporal information.
The layer-wise propagation rule of the TNA block at depth l
can thus be summarised as follows for the entire graph Gt âˆˆ
G(cid:48) with normalised adjacency matrix Ë†At :

t

t

= GCN ( Ë†At , H (lâˆ’1)
)
= GRU (H GCN
tâˆ’1 )

H GCN
t
(cid:0)W (l)
H GRU
t
t
H T N A(l)
t
t ) = H (l)
t = H T N A(l)
t

s Concat(H GCN

, H GRU

= Ïƒlr

t

T N A( Ë†At , H (l)

)(cid:1)

(3)

, H GRU
t

s

where W (l)
represents the weight matrix used to mix the
topological and temporal representations, and Ïƒlr is the leaky
ReLU activation function with a negative slope of 0.01.
2) Overall Model Architecture: As with normal GCN lay-
ers, TNA blocks can be stacked to aggregate information from
greater depth within a graph, with each additional block adding
one extra hop from which information can be aggregated for
a certain vertex. However, as our TNA blocks are recurrent,
information can also be aggregated from how connectivity
within these hops has evolved over time, instead of just their
present state. After extensive ablation studies (detailed in
Section V-A), we use the ï¬nal conï¬guration of the model
detailed in Figure 3. Our model contains two stacked TNA
blocks, to learn information from two hops within the temporal
neighbourhood. This is then passed to two independent GCN
layers which perform a ï¬nal aggregation of this temporal
representation. From these two layers, the ï¬nal representa-
tion matrix Zt is sampled using techniques from variational
inference, speciï¬cally the reparametrisation trick [33].

Variational Sampling - To create the ï¬nal representation ma-
trix Zt âˆˆ R|Vt |Ã—d , the output from the two GCN layers GCNÂµ
and GCNÏƒ are used to parametrise a unit Gaussian distribution
N , from which Zt is then sampled, rather than being explicitly
drawn. This is the same concept used in Variational Auto-
Encoders [33], and has previously been demonstrated to work
well for creating more robust and meaningful vertex level
representations [12], [34]. Our inference model used to create
the vertex representations of graph Gt , with adjacency matrix
At and identity matrix of At , Xt , can thus be described as :
|Vt |(cid:89)

N (zv |GCN Âµv , diag(GCN Ïƒ2
v )),

q(Zt |Xt , At ) =

(4)

v=1

where q is our approximation of the true and intractable
distribution we are interested in capturing â€“ p(At+1 |Zt ). Here,
both GCNÂµ and GCNÏƒ take input from two stacked TNA
layers as detailed in Figure 3.
Generative Model - To decode the information contained
within Zt , a generative model is created to explicitly predict
the new edges appearing in the next graph in the sequence.
Here, the inner-product between the latent representation is
used to directly predict At+1 :
|V |(cid:89)
|V |(cid:89)

p(At+1 |Zt ) =

p(At+1i,j |Ïƒs (zi zT

j )),

(5)

i=1

j=1

where At+1i,j represents elements from At+1 and z refers to
the rows of each vertex taken from Zt .
This generative model is one of the key advantages of our
approach, as it means that we have zero learnable parameters
in the decoder portion of the model. This is in contrast to
many competing approaches, which often require as many
parameters as in the encoder to create a decoder with the
desired functionality [19]. This results in our approach being
able to scale to signiï¬cantly larger graphs, with longer histories
than some of the competing approaches, whilst also being less
prone to over-ï¬tting to none-changing edges.

TNATNAGCNGCNZInner Productt={1...T-1}t=TEncoderDecoderTopology + Temporal LearningVariationalSamplingğœğœ‡D. Objective Function
To train the TNA model, and as is common for variational
methods [12], [33], we directly optimise the lower bound L
with regards to the model parameters:

L = Eq(Zt |Xt ,At )

logp(At+1 |Zt )
K L(q(Zt |At , Xt )||p(Zt )),

(cid:104)

(cid:105)âˆ’

Input

Algorithm 1: New edge predicition procedure
: The temporal graph G(cid:48) = {G1 , G2 , ..., GT }
Output: Mean AUC and AP scores for predicting new
edges for each graph in G(cid:48)
1 for all Gt âˆˆ G(cid:48) where t â‰¥ 3 do
Load and pre-process the graphs G1 , G2 , ..., GT
Create new model Î˜i (as shown in Figure 3)
Train Î˜i on sequence G1 , G2 , ..., Gtâˆ’1 , where each
graph is the input and used to predict the following
one
Predict new edges in Gt using Î˜i (Gtâˆ’1 ): Et \ Etâˆ’1
Store AUC and AP values
8 return Mean AUC and AP values over G(cid:48)

7 end

2
3
4

5
6

(6)

(7)

where K L() is the Kullback-Leibler distance between p and
q . We use a Gaussian prior as the distribution for p(Zt ).
In addition, we apply L2 regularization to our model pa-
rameters to help with over-ï¬tting, which is deï¬ned as:
|Î˜|(cid:88)

Lreg = Î»

Î˜2
i ,

where Î» is a scaling factor, set to 10âˆ’5 . Consequently, the
ï¬nal objective function for our model is:

i=1

Lf inal = L + Lreg .

(8)

E. Model Parameters and Training Procedure
After initial grid-searches, we empirically found two layers
of Temporal Neighbourhood Aggregation, followed by varia-
tional sampling, to yield the optimal performance, with the ï¬rst
layer comprising 32 ï¬lters, whilst the second having 16 ï¬lters.
For training the model, we empirically found using full-batch
gradient descent with the RMSProp algorithm, a learning rate
of 0.001 and 200 epochs to give the best results. Our model
has been implemented in PyTorch [7].

IV. EX PER IM EN TA L S E TU P

We detail the setup of our experimental evaluation, as well
as the baseline approaches and the datasets we use.

A. Evaluation Overview and Methodology
As the primary goal is to create vertex representations which
are better at encoding temporal change, we will be using
the task of future link prediction as our primary objective.
More formally, we are trying to maximise the probability of
P (Gt |G1 ...Gtâˆ’1 ). In the context of machine learning, this can
be deï¬ned as training a model from a temporal G(cid:48) using
G1 ...Gtâˆ’1 such that
it can predict
the new edges in Gt ,
Et \ Etâˆ’1 . The full training and evaluation process is detailed
in Algorithm 1. Many recent methods attempt to solve this
problem via vertex embedding similarity â€“ i.e. vertices with
more similar embeddings, according to some metric, are more
likely to be connected via an edge [5], [10], [12].
Graph edges are predicted as follows: given the learned ver-
tex embeddings, the future adjacency matrix is reconstructed
via the dot product of the embedding matrix A(cid:48)
This reconstructed adjacency matrix is compared with the true
graph to assess how well the embedding is able to reconstruct
the future graph.

t+1 = Ïƒ(ZtZ T
t ).

B. Performance Metrics
As one can consider the task of link prediction to be a binary
classiï¬cation problem (an edge can only be present or not),
we make use of two standard binary classiï¬cation metrics:
â€¢ Area Under the Receiver Operating Characteristic Curve
(AUC) â€“ The ratio between the True Positive Rate (TPR)
and False Positive Rate (FPR) measured at various clas-
siï¬cation thresholds.
â€¢ Mean Average Precision (AP) â€“ Across the set of test
edges: AP = T P
T P +F P , where T P denotes the number
of true positives the model predicts, and F P denotes the
number of false positives.
For both of the chosen metrics, a larger value indicates more
correctly predicted edges.

C. Datasets
When performing our experimental evaluation, we employ
the empirical datasets presented in Table II. The graphs
represent a range of domains, sizes and temporal complexities.
Bitcoin-Alpha (Bitcoina) - Representing a trust network
within a platform entitled Bitcoin Alpha, where edges are
formed as users interact and rate each others reputation. The
graph covers a range of edges formed between 8th October
2010 and 22nd January 2016, which we partition into 62
monthly snapshots. The task of new edge prediction is thus
analogous to predicting if two users are going to interact within
the next month.
Wiki-Vote (Wiki) - Representing a vote of escalating user
privileges between users and administrators on the Wikipedia
website. The graph covers a range of edges formed between
28th March 2004 and 6th January 2008, which we partition
into 34 monthly snapshots. The task of new edge prediction
within this data is analogous to predicting if two users are
going to vote for each other within the next week.
UCI-Messages (UCI) - Representing private messages sent
between users on the University of California Irvine social
network platform. The graph covers a range of edges formed
between 15th April 2004 and 25th October 2004, which we
partition into 27 weekly snapshots. The task of new edge

Dataset

|V |

|E |

First Edge

Last Edge

Num Snapshots

# New Edges

Reference

Bitcoin-Alpha (Bitcoina)
Wiki-Vote (Wiki)
UC Irvine Messages (UCI)

3,783
7,115
1,899

24,186
103,689
20,296

08/09/2010
28/02/2005
15/04/2004

22/01/2016
06/01/2008
25/08/2004

62
34
27

227
2963
513

[35]
[35]
[36]

TABLE II: Empirical graph datasets, where # New Edges is the average number of new edges added between time points.

prediction would represent the likelihood that two users will
exchange messages with each other over the next week.
1) Synthetic Datasets: In addition, we use two synthetic
datasets: a Stochastic Block Model (SBM) graph and a ran-
domly perturbed version of the Cora dataset (R-Cora).
SBM - A random graph of 3,000 vertices, which evolves
over 30 time points using the SBM algorithm [37]. The graph
contains 3 communities and at each time point, 20 vertices
will evolve by switching from one community to another.
R-Cora - To create this synthetic dataset, we take the
original Cora dataset representing a citation network, and
perturb the graph using the random rewire method [38], [39].
The rewiring process alters a given source graphâ€™s degree
distribution by randomly altering the source and target of a
set number of edges. During this rewiring process, it is not
guaranteed that the source or target of the edge will be altered,
which indeed is not always possible due to the topology
of the graph. Also, the rewiring process does not change
the total number of edges or vertices within the graph. We
employ Erd Ëos rewiring, i.e. the resulting topology of the graph
begins to resemble a Erd Ëos-R Â´enyi graph, where the edges are
uniformly distributed between vertices.

D. Baseline Approaches
We compare our approach against a variety of state-of-
the-art graph representation learning techniques, both static
and dynamic. We choose the baselines which compare most
directly with our proposed approach, meaning we opt for
comparators which take advantage of deep neural networks
to create vertex embeddings.
â€¢ GAE [12]: A non-probabilistic Graph Convolutional
Auto-encoder (GAE), where the model is trained on Gtâˆ’1
and then directly predicts new edges in Gt .
â€¢ GVAE [12]: A Graph Variational Convolutional Auto-
encoder (GVAE), trained in the same manner as the GAE.
â€¢ TO-GAE [34]: A GAE model training procedure which
enables temporal offset reconstruction, where the model
is trained on Gtâˆ’2 to predict Gtâˆ’1 . Gtâˆ’1 is subsequently
used as input and the ability to predict Gt is measured.
â€¢ TO-GVAE [34]: A GVAE model trained using the tem-
poral offset reconstruction method.
â€¢ DynAE [19]: A non-convolutional graph embedding
model, similar to SDNE [17], extended to temporal
graphs by concatenating the rows of the past graphs
together before being passed into the model.
â€¢ DynRNN [19]: A non-convolutional graph embedding
model, where stacked LSTM units are used to encode
the temporal graph directly. The approach also requires a

decoder model, also comprised of stacked LSTM units,
to reconstruct the next graph from the embedding.
â€¢ DynAERNN [19]2 : A combination of the previous two
models, where a dense auto-encoder is used to learn a
compressed representation which is passed to stacked
LSTM units for temporal learning. It requires a large
decoder, with both dense and LSTM layers, to predict
the next graph. The E-LSTM-D approach [41] is also
extremely similar to this model.
â€¢ D-GCN: [20], [21]: A dynamic GCN, similar to ap-
proaches proposed in [20] and [21]. Here, three stacked
GCN layers are used to capture structural information
with an LSTM unit used to learn temporal information
and produce the ï¬nal embeddings. To directly predict
the next graph, we use an inner-product decoder on the
embedding matrix.
We attempted to compare with GCN-GAN [22] and GC-
LSTM [23], but we were unable to get them to scale to the
size of graphs we are using for our experimentation.
E. Experimental Environment
Experimentation was performed on a system with 2 *
NVIDIA Titan Xp GPUs, 2.3GHz Intel Xeon E5-2650 v3,
64GB RAM, with Ubuntu Server 18.04 LTS, Python 3.7,
CUDA 10.1, CuDNN v7.4 and PyTorch 1.1.

V. R E SU LT S

We evaluate our TNA approach using comparisons against
state-of-the-art approaches and ablation studies using well-
established datasets (Section IV-C).
A. Ablation Study
One of the major contributions of our work is highlighting
how each component of our TNA model is crucial in producing
good temporal embeddings. To highlight this, Table IV shows
how adding components of the model sequentially affects the
performance of predicting new edges in the ï¬nal graph of the
Bitcoina dataset. It is important to note that adding temporal
information from both the ï¬rst and second hop neighbourhood
(Model TTV) lifts both AUC and AP scores by approximately
10% versus just ï¬rst hop temporal information (Model TGV).
This supports our hypothesis that a vertex requires temporal
information from more than just its ï¬rst-order neighbourhood
in order to predict future edges. The ablation study also
demonstrates that, with a modest increase in the number of
parameters, the temporal models are able to exploit the rich
information available in the graphâ€™s past evolution to much
more accurately predict future edges.

2 For the Dyn* family of algorithms, we use the implementations as
provided by the authors as part of their DynamicGEM package [40].

Dataset

Approach

GAE
GVAE
TO-GAE
TO-GVAE
DynAE
DynRNN
DynAERNN
D-GCN

TNA

GAE
GVAE
TO-GAE
TO-GVAE
DynAE
DynRNN
DynAERNN
D-GCN

TNA

GAE
GVAE
TO-GAE
TO-GVAE
DynAE
DynAERNN
D-GCN

TNA

25%

0.466 Â± 0.025
0.577 Â± 0.048
0.551 Â± 0.053
0.598 Â± 0.048
0.281 Â± 0.080
0.181 Â± 0.081
0.093 Â± 0.090
0.622 Â± 0.084
0.665 Â± 0.067
0.561 Â± 0.075
0.571 Â± 0.079
0.601 Â± 0.059
0.582 Â± 0.072
0.234 Â± 0.066
0.161 Â± 0.019
0.033 Â± 0.032
0.508 Â± 0.041
0.694 Â± 0.077
0.491 Â± 0.035
0.580 Â± 0.024
0.537 Â± 0.052
0.599 Â± 0.028
0.354 Â± 0.034
0.183 Â± 0.024
0.628 Â± 0.160
0.674 Â± 0.034

Bitcoina

UCI

Wiki

AUC

50%

0.497 Â± 0.042
0.602 Â± 0.046
0.566 Â± 0.053
0.620 Â± 0.045
0.247 Â± 0.065
0.170 Â± 0.059
0.071 Â± 0.066
0.572 Â± 0.080
0.698 Â± 0.075
0.600 Â± 0.075
0.606 Â± 0.074
0.633 Â± 0.061
0.614 Â± 0.069
0.168 Â± 0.076
0.176 Â± 0.024
0.021 Â± 0.025
0.555 Â± 0.071
0.749 Â± 0.073
0.487 Â± 0.038
0.573 Â± 0.018
0.556 Â± 0.049
0.595 Â± 0.021
0.325 Â± 0.041
0.179 Â± 0.026
0.591 Â± 0.115
0.644 Â± 0.044

100%

0.531 Â± 0.127
0.620 Â± 0.083
0.576 Â± 0.124
0.631 Â± 0.081
0.209 Â± 0.071
0.155 Â± 0.066
0.048 Â± 0.054
0.519 Â± 0.144
0.775 Â± 0.110
0.606 Â± 0.092
0.619 Â± 0.065
0.625 Â± 0.087
0.624 Â± 0.062
0.128 Â± 0.067
0.159 Â± 0.048
0.013 Â± 0.019
0.565 Â± 0.068
0.764 Â± 0.071
0.502 Â± 0.040
0.563 Â± 0.024
0.552 Â± 0.048
0.579 Â± 0.029
0.244 Â± 0.089
0.127 Â± 0.056
0.563 Â± 0.087
0.634 Â± 0.050

25%

0.613 Â± 0.031
0.634 Â± 0.043
0.694 Â± 0.038
0.646 Â± 0.044
0.435 Â± 0.012
0.388 Â± 0.014
0.326 Â± 0.022
0.697 Â± 0.058
0.762 Â± 0.048
0.661 Â± 0.066
0.585 Â± 0.059
0.682 Â± 0.053
0.590 Â± 0.057
0.436 Â± 0.019
0.365 Â± 0.016
0.314 Â± 0.005
0.605 Â± 0.045
0.702 Â± 0.073
0.642 Â± 0.029
0.598 Â± 0.032
0.700 Â± 0.032
0.613 Â± 0.036
0.448 Â± 0.009
0.342 Â± 0.005
0.745 Â± 0.104
0.759 Â± 0.025

AP

50%

0.643 Â± 0.042
0.654 Â± 0.040
0.701 Â± 0.038
0.665 Â± 0.040
0.442 Â± 0.012
0.388 Â± 0.011
0.320 Â± 0.016
0.661 Â± 0.058
0.792 Â± 0.054
0.688 Â± 0.060
0.621 Â± 0.063
0.705 Â± 0.050
0.624 Â± 0.062
0.435 Â± 0.021
0.370 Â± 0.016
0.312 Â± 0.004
0.653 Â± 0.066
0.763 Â± 0.075
0.621 Â± 0.033
0.589 Â± 0.025
0.697 Â± 0.027
0.604 Â± 0.029
0.463 Â± 0.016
0.341 Â± 0.006
0.686 Â± 0.094
0.740 Â± 0.032

100%

0.681 Â± 0.093
0.670 Â± 0.068
0.715 Â± 0.085
0.631 Â± 0.081
0.439 Â± 0.023
0.393 Â± 0.022
0.318 Â± 0.012
0.623 Â± 0.107
0.849 Â± 0.079
0.689 Â± 0.079
0.625 Â± 0.060
0.699 Â± 0.076
0.627 Â± 0.060
0.433 Â± 0.017
0.369 Â± 0.029
0.312 Â± 0.003
0.656 Â± 0.072
0.783 Â± 0.067
0.617 Â± 0.032
0.572 Â± 0.029
0.668 Â± 0.044
0.583 Â± 0.034
0.467 Â± 0.013
0.329 Â± 0.012
0.629 Â± 0.089
0.736 Â± 0.039

|Î˜|

121K
122K
120K
122K
4.16M
69.9M
6.98M
125K

133K

61K
62K
61K
62K
2.28M
21.8M
4.15M
64K

72K

228K
229K
228K
229K
7.5M
11.9M
231K

239K

TABLE III: Next graph prediction results presented as mean values with standard deviation when predicting at various
percentages of the length of the time-sequence. A bold value indicates the highest score for that metric. The number of
parameters required by each model for the speciï¬c datasets are also included.

(a) AUC score on Wiki

(b) AP score on Wiki

(c) AUC score on UCI

(d) AP score on UCI

Fig. 4: AUC and AP for the Wiki and UCI datasets when predicting new edges n number of time points away from the end
of the training sequence. Results presented as the mean of three uniquely trained models, each with a different random seed.

Approach

AUC

AP

GGG
GGV
TGV
TTV
TTV/LN

0.574
0.721
0.772
0.863
0.927

0.747
0.705
0.809
0.916
0.932

|Î˜|

121K
122K
130K
132K
132K

TTV/LN/SC (TNA)

0.977

0.976

133K

TABLE IV: Ablation study results on the Bitcoina dataset. G
is a GCN layer, V is a varitonal sampling layer, T is a GCN
+ GRU layer, LN is Layer Norm and SC is a skip-connection.
|Î˜| is the total number of learnable parameters in the model.

B. Next Graph Link Prediction
As the main focus of our model, we present results for
predicting new edges in the next temporal graph, using the
procedure detailed in Algorithm 1, in Table III3 . The table
shows that TNA signiï¬cantly outperforms the baseline ap-
proaches when predicting new edges in the next graph at all
points along the time series. Compared with the Dyn* family
of approaches, it is striking to note the signiï¬cant number of
parameters required by the models (often well over an order of
magnitude more) and their poor performance in predicting new
edges. We believe it is highly likely that this family of models
is using the extra parameters to over-ï¬t to the edges that do
not change over time, resulting in bad predictive capability for

3DynRNN is missing for the Wiki dataset as it could not ï¬t in GPU memory.

12345Graph Number Predicted Past Last Training Graph0.50.60.70.80.9AUC ScoreTNAD-GCN12345Graph Number Predicted Past Last Training Graph0.50.60.70.80.9AP ScoreTNAD-GCN12345Graph Number Predicted Past Last Training Graph0.450.500.550.600.650.700.750.80AUC ScoreTNAD-GCN12345Graph Number Predicted Past Last Training Graph0.600.650.700.750.800.85AP ScoreTNAD-GCNDataset

Approach

AUC

AP

Dataset

Approach

AUC

AP

SBM

R-Cora

GAE
GVAE
TO-GAE
TO-GVAE
DynAE
DynRNN
DynAERNN
D-GCN

TNA

GAE
GVAE
TO-GAE
TO-GVAE
DynAE
DynRNN
DynAERNN
D-GCN

TNA

0.505 Â± 0.018
0.500 Â± 0.012
0.504 Â± 0.017
0.500 Â± 0.012
0.023 Â± 0.003
0.039 Â± 0.005
0.008 Â± 0.000
0.458 Â± 0.017
0.502 Â± 0.024
0.501 Â± 0.015
0.491 Â± 0.011
0.500 Â± 0.013
0.490 Â± 0.011
0.356 Â± 0.001
0.308 Â± 0.011
0.201 Â± 0.000
0.502 Â± 0.011
0.493 Â± 0.012

0.451 Â± 0.009
0.503 Â± 0.011
0.451 Â± 0.008
0.503 Â± 0.011
0.431 Â± 0.008
0.348 Â± 0.009
0.308 Â± 0.000
0.458 Â± 0.017
0.502 Â± 0.017
0.500 Â± 0.0100
0.494 Â± 0.002
0.502 Â± 0.009
0.494 Â± 0.011
0.479 Â± 0.003
0.381 Â± 0.011
0.346 Â± 0.000
0.500 Â± 0.008
0.493 Â± 0.012

TABLE V: Next graph prediction results on sythnetic graphs
presented as mean values with standard deviation when pre-
dicting at each point in the time series.

the ones that do. It is also interesting to note that, compared
with the D-GCN approach, TNA is better able to capture the
dependences needed for good long-term prediction. For two
datasets our model improves the past graph evolution data it
has to learn from. This is demonstrated by the increasing AUC
and AP scores for the Bitcoina and UCI datasets. However,
all approaches struggle on the synthetic datasets due to the
inherent random nature, as seen in Table V.

C. Full Graph Reconstruction
To measure the ability of the representations learned by
the TNA model to be used as general purpose embeddings,
we look at the problem of future graph reconstruction. Here,
the performance of the model at predicting the presence of
edges in the full graph Gt (given G1 ..Gtâˆ’1 ) is measured â€“
highlighting how we do not sacriï¬ce performance at predicting
existing edges. This will allow us to investigate the ability of
the model to predict not only new edges, but that existing
edges have not been removed. As before, a new model is
trained to predict the ï¬nal graph in the sequence given all
previous time points, with the ï¬nal results presented as the
mean over all graphs in the sequence. However, instead of
predicting edges which have appeared since the last time point,
here the results are for a balanced set of random sampled
positive and negative edges in Et which may or may not
include ones formed since the previous time point.
The results for this experiment are presented in Table VI
where for the sake of brevity, we compare with only the
temporal baselines. It is obvious that many of the baselines,
especially the Dyn* family of approaches perform much
better at predicting existing edges than new ones. This further
suggests that they are utilising their larger set of parameters to,
in some way, over-ï¬t to edges which have been in the graph
for a longer length of time, which form the vast majority.
However despite this, our TNA approach still performs well at

Bitcoina

UCI

Wiki

DynAE
DynRNN
DynAERNN
D-GCN

TNA

DynAE
DynRNN
DynAERNN
D-GCN

TNA

DynAE
DynAERNN
D-GCN

TNA

0.830 Â± 0.068
0.922 Â± 0.059
0.968 Â± 0.057
0.919 Â± 0.021
0.932 Â± 0.024
0.905 Â± 0.061
0.957 Â± 0.015
0.988 Â± 0.014
0.829 Â± 0.019
0.821 Â± 0.015
0.765 Â± 0.088
0.882 Â± 0.072
0.905 Â± 0.019
0.919 Â± 0.014

0.844 Â± 0.050
0.937 Â± 0.039
0.981 Â± 0.034
0.934 Â± 0.016
0.945 Â± 0.018
0.908 Â± 0.055
0.954 Â± 0.010
0.993 Â± 0.009
0.862 Â± 0.014
0.847 Â± 0.012
0.795 Â± 0.062
0.934 Â± 0.037
0.936 Â± 0.015
0.945 Â± 0.007

TABLE VI: Results for predicting both new and old edges
in the ï¬nal graph in the sequence, presented as a mean and
standard deviation over the whole time sequence. A bold
value indicates the highest score for that metric. TNA remains
competitive with, and even beats many baseline approaches
with a much greater number of parameters.

this task, displaying comparable performance with the baseline
approaches and even outperforming them on the Wiki dataset.
This further strengthens the argument that having recurrence at
each hop in the neighbourhood aggregation produces a better
representation, whilst requiring fewer parameters.

D. Future Graph Evolution
For our ï¬nal experiment, we investigate how TNA performs
when predicting new edges further into the future than the
next graph. We train the models on 70% of the available
temporal history, then predict new edges and compare with the
remaining ground truth data. To achieve this, we feed the graph
predicted by the models as the next graph in the sequence back
into the model, which is subsequently used to predict the next
graph. This is similar to using RNNs as generative models to
produce text data [42] and can be seen as a combination of
both the previous tasks. Figure 4 displays the results for this
task, where we compare with the closet baseline from Section
V-B. The results show how TNA is better able to predict new
edges into the future, emphasising its capability to learn a good
temporal representation for the vertices.

V I . CONC LU S ION

Many real-world graph datasets have rich and complex tem-
poral information available which is disregard by the majority
of the current approaches for creating vertex representations.
In this paper, we have introduced the Temporal Neighbour-
hood Aggregation model for representation learning on large,
complex temporal graphs. Our approach demonstrates excel-
lent performance through extensive experimental evaluation,
beating several competing temporal and static models, when
predicting future edges not seen in the training data. The
TNA model can learn complex temporal patterns present at

multiple depths within a vertices neighbourhood, creating the
ï¬nal vertex representation via the use of variational sampling.
For future work, we will investigate replacing the GCN in
our model with an approach designed for inductive learning
[30] to allow for training on even larger graph datasets, as
well as enabling vertex arrival to be modelled. We also plan
to experiment using the learned representations for additional
tasks, such as temporal classiï¬cation.

ACKNOW LEDG EM EN T

We gratefully acknowledge the support of NVIDIA Corpo-
ration with the donation of the GPU used for this research.
Additionally we thank the Engineering and Physical Sciences
Research Council UK (EPSRC) for funding.

R E FERENC E S
[1] T. N. Kipf and M. Welling, â€œSemi-supervised classiï¬cation with graph
convolutional networks,â€ in International Conference on Learning Rep-
resentations (ICLR), 2017.
[2] R. Yang, Y. Bai, Z. Qin, and T. Yu, â€œEgonet: identiï¬cation of human
disease ego-network modules,â€ BMC genomics, vol. 15, no. 1, p. 314,
2014.
[3] Z. Wu, B. Ramsundar, E. N. Feinberg, J. Gomes, C. Geniesse, A. S.
Pappu, K. Leswing, and V. Pande, â€œMoleculenet: a benchmark for
molecular machine learning,â€ Chemical science, vol. 9, no. 2, pp. 513â€“
530, 2018.
[4] P. Battaglia, R. Pascanu, M. Lai, D. J. Rezende et al., â€œInteraction
networks for learning about objects, relations and physics,â€ in Advances
in Neural Information Processing Systems, 2016, pp. 4502â€“4510.
[5] A. Grover and J. Leskovec, â€œnode2vec : scalable feature learning for
networks,â€ International Conference on Knowledge Discovery and Data
Mining, 2016.
[6] P. Goyal and E. Ferrara, â€œGraph embedding techniques, applications,
and performance: a survey,â€ arXiv preprint arXiv:1705.02801, 2017.
[7] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,
T. Killeen, Z. Lin, N. Gimelshein, L. Antiga et al., â€œPytorch: An
imperative style, high-performance deep learning library,â€ in Advances
in Neural Information Processing Systems, 2019, pp. 8024â€“8035.
[8] M. Belkin and P. Niyogi, â€œLaplacian eigenmaps and spectral techniques
for embedding and clustering,â€ Advances in neural information process-
ing systems, pp. 585â€“591, 2002.
[9] A. Ahmed, N. Shervashidze, S. Narayanamurthy, V. Josifovski, and
A. J. Smola, â€œDistributed large-scale natural graph factorization,â€ In-
ternational conference on World Wide Web, pp. 37â€“48, 2013.
[10] B. Perozzi, R. Al-Rfou, and S. Skiena, â€œDeepWalk: online learning of
social representations,â€ International Conference on Knowledge Discov-
ery and Data Mining, 2014.
[11] P. Baldi, â€œAutoencoders, unsupervised learning, and deep architectures,â€
in Proceedings of ICML workshop on Unsupervised and Transfer
Learning, 2012, pp. 37â€“49.
[12] T. N. Kipf and M. Welling, â€œVariational graph auto-encoders,â€ arXiv
preprint arXiv:1611.07308, 2016.
[13] S. Pandhre, H. Mittal, M. Gupta, and V. N. Balasubramanian, â€œStwalk:
learning trajectory representations in temporal graphs,â€ in Proceedings
of the ACM India Joint International Conference on Data Science and
Management of Data. ACM, 2018, pp. 210â€“219.
[14] W. Yu, W. Cheng, C. C. Aggarwal, K. Zhang, H. Chen, and W. Wang,
â€œNetwalk: A ï¬‚exible deep embedding approach for anomaly detection in
dynamic networks,â€ in Iternational Conference on Knowledge Discovery
& Data Mining. ACM, 2018, pp. 2672â€“2681.
[15] G. H. Nguyen, J. B. Lee, R. A. Rossi, N. K. Ahmed, E. Koh,
and S. Kim, â€œContinuous-time dynamic network embeddings,â€ in 3rd
International Workshop on Learning Representations for Big Networks
(WWW BigNet), 2018.
[16] P. Goyal, N. Kamra, X. He, and Y. Liu, â€œDyngem: Deep embedding
method for dynamic graphs,â€ arXiv preprint arXiv:1805.11273, 2018.
[17] D. Wang, P. Cui, and W. Zhu, â€œStructural deep network embedding,â€
in International Conference on Knowledge Discovery and Data Mining.
ACM, 2016, pp. 1225â€“1234.

[18] T. Chen, I. Goodfellow, and J. Shlens, â€œNet2net: Accelerating learning
via knowledge transfer,â€ arXiv preprint arXiv:1511.05641, 2015.
[19] P. Goyal, S. R. Chhetri, and A. Canedo, â€œdyngraph2vec: Captur-
ing network dynamics using dynamic graph representation learning,â€
Knowledge-Based Systems, 2019.
[20] F. Manessi, A. Rozza, and M. Manzo, â€œDynamic graph convolutional
networks,â€ Pattern Recognition, p. 107000, 2019.
[21] Y. Seo, M. Defferrard, P. Vandergheynst, and X. Bresson, â€œStructured
sequence modeling with graph convolutional recurrent networks,â€ in
International Conference on Neural Information Processing. Springer,
2018, pp. 362â€“373.
[22] K. Lei, M. Qin, B. Bai, G. Zhang, and M. Yang, â€œGcn-gan: A non-linear
temporal link prediction model for weighted dynamic networks,â€ in
IEEE INFOCOM 2019-IEEE Conference on Computer Communications.
IEEE, 2019, pp. 388â€“396.
[23] J. Chen, X. Xu, Y. Wu, and H. Zheng, â€œGc-lstm: Graph convo-
lution embedded lstm for dynamic link prediction,â€ arXiv preprint
arXiv:1812.04206, 2018.
[24] A. Pareja, G. Domeniconi, J. Chen, T. Ma, T. Suzumura, H. Kanezashi,
T. Kaler, and C. E. Leisersen, â€œEvolvegcn: Evolving graph convolutional
networks for dynamic graphs,â€ arXiv preprint arXiv:1902.10191, 2019.
[25] H. Yao, X. Tang, H. Wei, G. Zheng, and Z. Li, â€œRevisiting spatial-
temporal similarity: A deep learning framework for trafï¬c prediction,â€
in AAAI Conference on Artiï¬cial Intelligence, 2019.
[26] Y. Li, R. Yu, C. Shahabi, and Y. Liu, â€œDiffusion convolutional re-
current neural network: Data-driven trafï¬c forecasting,â€ arXiv preprint
arXiv:1707.01926, 2017.
[27] S. Hochreiter and J. Schmidhuber, â€œLong short-term memory,â€ Neural
Computation, vol. 9, no. 8, pp. 1735â€“1780, 1997.
[28] K. Cho, B. Van Merri Â¨enboer, C. Gulcehre, D. Bahdanau, F. Bougares,
H. Schwenk, and Y. Bengio, â€œLearning phrase representations using
rnn encoder-decoder for statistical machine translation,â€ arXiv preprint
arXiv:1406.1078, 2014.
[29] J. Chen, T. Ma, and C. Xiao, â€œFastgcn: fast
learning with graph
convolutional networks via importance sampling,â€ arXiv preprint
arXiv:1801.10247, 2018.
[30] W. Hamilton, Z. Ying, and J. Leskovec, â€œInductive representation
learning on large graphs,â€ in Advances in Neural Information Processing
Systems, 2017, pp. 1024â€“1034.
[31] J. L. Ba, J. R. Kiros, and G. E. Hinton, â€œLayer normalization,â€ arXiv
preprint arXiv:1607.06450, 2016.
[32] K. He, X. Zhang, S. Ren, and J. Sun, â€œDeep residual learning for image
recognition,â€ in Proceedings of the IEEE conference on computer vision
and pattern recognition, 2016, pp. 770â€“778.
[33] D. P. Kingma and M. Welling, â€œAuto-encoding variational bayes,â€ arXiv
preprint arXiv:1312.6114, 2013.
[34] S. Bonner, J. Brennan, I. Kureshi, G. Theodoropoulos, A. S. McGough,
and B. Obara, â€œTemporal graph offset reconstruction: Towards tempo-
rally robust graph representation learning,â€ in 2018 IEEE International
Conference on Big Data (Big Data).
IEEE, 2018, pp. 3737â€“3746.
[35] J. Leskovec and A. Krevl, â€œSNAP Datasets: Stanford large network
dataset collection,â€ http://snap.stanford.edu/data, Jun. 2014.
[36] J. Kunegis, â€œKonect: the koblenz network collection,â€ in Proceedings of
the 22nd International Conference on World Wide Web. ACM, 2013,
pp. 1343â€“1350.
[37] B. Karrer and M. E. Newman, â€œStochastic blockmodels and community
structure in networks,â€ Physical review E, vol. 83, no. 1, p. 016107,
2011.
[38] S. Bonner, J. Brennan, G. Theodoropoulos, I. Kureshi, and A. S.
McGough, â€œDeep topology classiï¬cation: A new approach for massive
graph classiï¬cation,â€ in International Conference on Big Data.
IEEE,
2016, pp. 3290â€“3297.
[39] S. Bonner, J. Brennan, I. Kureshi, M. Stephen, and G. Theodoropoulos,
â€œEfï¬cient comparison of massive graphs through the use of graph
ï¬ngerprints,â€ in KDD Workshop on Mining and Learning with Graphs
(MLG), 2016.
[40] P. Goyal, S. R. Chhetri, N. Mehrabi, E. Ferrara, and A. Canedo,
â€œDynamicgem: A library for dynamic graph embedding methods,â€ arXiv
preprint arXiv:1811.10734, 2018.
[41] J. Chen, J. Zhang, X. Xu, C. Fu, D. Zhang, Q. Zhang, and Q. Xuan,
â€œE-lstm-d: A deep learning framework for dynamic network link pre-
diction,â€ arXiv preprint arXiv:1902.08329, 2019.

[42] I. Sutskever, J. Martens, and G. E. Hinton, â€œGenerating text with
recurrent neural networks,â€ in Proceedings of the 28th International

Conference on Machine Learning (ICML-11), 2011, pp. 1017â€“1024.

